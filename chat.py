import os
import json
import faiss
import requests
from sentence_transformers import SentenceTransformer
from config import INDEX_DIR, FAISS_INDEX_FILE, META_FILE, API_DEF_FILE, EMBEDDING_MODEL
from logger import logger

# åŠ è½½æœ¬åœ° api.json å®šä¹‰
def load_api_defs():
    with open(API_DEF_FILE, encoding='utf-8') as f:
        return json.load(f)

# åŠ è½½ FAISS ç´¢å¼•å’Œå…ƒæ•°æ®
def load_index_and_meta():
    index_path = FAISS_INDEX_FILE
    meta_path  = META_FILE
    if not os.path.exists(index_path) or not os.path.exists(meta_path):
        logger.error("FAISS ç´¢å¼•æˆ–å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ chunker_indexer.py")
        return None, None
    # è¯»å–ç´¢å¼•
    index = faiss.read_index(index_path)
    # åŠ è½½ meta åˆ—è¡¨
    with open(meta_path, encoding='utf-8') as f:
        meta = json.load(f)
    return index, meta

# åŸºäºæŸ¥è¯¢æ£€ç´¢ top_k ä¸ªç›¸å…³ chunk æ–‡æœ¬
def retrieve_context(query, index, meta, top_k=3):
    # åˆå§‹åŒ–å¤šè¯­ç§ç¼–ç å™¨
    embedder = SentenceTransformer(EMBEDDING_MODEL)
    # å¯¹æŸ¥è¯¢åšå‘é‡åŒ–
    qvec = embedder.encode([query], convert_to_numpy=True)
    # ç´¢å¼•æœç´¢ï¼ˆå†…ç§¯è¿‘ä¼¼ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    D, I = index.search(qvec, top_k)
    contexts = []
    for idx in I[0]:
        # ä» meta ä¸­è·å–å¯¹åº”æ–‡æœ¬
        entry = meta[idx]
        contexts.append(entry['text'])
    return contexts

# è°ƒç”¨ LLM æ¥å£ç”Ÿæˆå›ç­”ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
def answer_query(query, contexts, api_defs):
    ep = api_defs["summarize"]
    url = f"{ep['url']}?key={ep['api_key']}"
    # æŠŠä¸Šä¸‹æ–‡å’Œé—®é¢˜æ‹¼è¿›ä¸€ä¸ªæ–‡æœ¬é‡Œ
    context_str = "\n".join(contexts)
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ AI åŠ©æ‰‹ï¼ŒåŸºäºä»¥ä¸‹æ–‡ç« æ®µè½å›ç­”ï¼Œä¸è¦ç¼–é€ ï¼š\n\n"
        f"{context_str}\n\næé—®ï¼š{query}"
    )
    payload = {
        "contents": [
            {
                "parts": [
                    { "text": prompt }
                ]
            }
        ]
    }
    resp = requests.post(url, json=payload, timeout=60)
    resp.raise_for_status()
    j = resp.json()
    return j["candidates"][0]["content"]["parts"][0]["text"].strip()


def main():
    logger.info("Chat æ¨¡å—å¯åŠ¨")
    api_defs = load_api_defs()
    index, meta = load_index_and_meta()
    if index is None or meta is None:
        return

    # CLI å¾ªç¯
    print("è¾“å…¥é—®é¢˜å¹¶å›è½¦ (è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º)ï¼š")
    while True:
        query = input("> ").strip()
        if query.lower() in ("exit", "quit"):
            print("é€€å‡º Chatã€‚")
            break
        if not query:
            continue
        try:
            # æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
            contexts = retrieve_context(query, index, meta, top_k=3)
            # ç”Ÿæˆå¹¶æ‰“å°å›ç­”
            answer = answer_query(query, contexts, api_defs)
            print(f"\nğŸ“ å›ç­”ï¼š\n{answer}\n")
        except Exception as ex:
            logger.error(f"å›ç­”è¿‡ç¨‹å‡ºé”™ï¼š{ex}")
            print(f"âš ï¸ å›ç­”å¤±è´¥ï¼š{ex}")

    logger.info("Chat æ¨¡å—ç»“æŸ")

if __name__ == "__main__":
    main()
