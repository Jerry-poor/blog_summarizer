{
  "title": "Introducing HELMET",
  "link": "https://huggingface.co/blog/helmet",
  "published": "2025-04-16",
  "summary": "**Abstract:**\n\n评估长文本语言模型（LCLMs）既具有挑战性又至关重要。Evaluating long-context language models (LCLMs) is challenging but important. 现有评估过度依赖合成任务。Existing evaluations overly rely on synthetic tasks. 我们引入HELMET，旨在为LCLMs构建多样、可控和可靠的评估。We introduce HELMET to craft diverse, controllable, and reliable evaluations for LCLMs. 相较于现有基准，HELMET进行了关键改进。HELMET includes key improvements over existing benchmarks. 结果表明，LCLMs在真实世界任务中仍有很长的路要走，模型性能随长度和任务复杂性增加而下降。Results show that LCLMs still have a long way to go on real-world tasks, and models degrade with increasing lengths and task complexity. HELMET促进对LCLMs能力的更全面评估，并推动未来发展。HELMET facilitates a more comprehensive assessment of LCLM capabilities and promotes future developments. 可通过 `https://github.com/princeton-nlp/HELMET` 访问代码和数据。 Code & Data is available at `https://github.com/princeton-nlp/HELMET`.\n\n**Abstract**\n\n大型语言模型(LLMs)极大地扩展了其上下文窗口，但现有评测体系存在局限性。\nLarge language models (LLMs) have significantly expanded their context window, but existing evaluation methods have limitations.\n\n现有基准测试显示出反直觉的趋势，例如较小的模型优于较大的模型 (例如，Llama-3 8B > 70B)。\nExisting benchmarks show counterintuitive trends, such as smaller models outperforming larger ones (e.g., Llama-3 8B > 70B).\n\n现有的长上下文语言模型 (LCLMs) 的评测过度依赖于合成任务，且模型开发者经常使用不同的数据集进行评测。\nExisting evaluations of long-context language models (LCLMs) overly rely on synthetic tasks, and model developers often evaluate on different sets of datasets.\n\n我们提出了HELMET (How to Evaluate Long-Context Models Effectively and Thoroughly)，一个全面的LCLMs评测基准，以解决现有基准的多样性、可控性和可靠性问题。\nWe propose HELMET (How to Evaluate Long-Context Models Effectively and Thoroughly), a comprehensive benchmark for evaluating LCLMs that addresses the diversity, controllability, and reliability issues of existing benchmarks.\n\n通过评估59个最新的LCLMs，我们发现，跨多样化应用评估模型的能力至关重要，并且前沿的LCLMs在复杂任务上仍然存在局限性。\nBy evaluating 59 recent LCLMs, we find that it is crucial to evaluate models across diverse applications to understand their capabilities, and frontier LCLMs are still limited on complex tasks.\n\n**Abstract:**\n\n长文本语言模型(LCLMs)的常用评估方法是困惑度或如大海捞针(NIAH)等合成任务。\nA common practice for evaluating long-context language models (LCLMs) is to use perplexity or synthetic tasks, such as needle-in-a-haystack (NIAH).\n\n然而，近期研究表明，困惑度与下游性能相关性不佳(Fang et al., 2024)。\nHowever, recent works have shown that perplexity does not correlate well with downstream performance (Fang et al., 2024).\n\nNIAH等简单合成任务与实际任务相关性差，而更复杂的合成任务相关性更高。\nSimple synthetic tasks, such as NIAH, do not correlate well with real-world performance, but the more complex synthetic tasks achieve higher correlation with real-world tasks.\n\n现有实际应用基准测试如ZeroScrolls, LongBench, 和InfiniteBench仍存在下游任务覆盖不足、测试长度受限和指标不可靠等问题。\nExisting benchmarks with realistic applications such as ZeroScrolls, LongBench, and InfiniteBench still face limitations: insufficient task coverage, inadequate length, and unreliable metrics.\n\n因此，我们提出了HELMET以解决这些问题，提供全面的LCLM评估。\nThus, we propose HELMET to address these limitations and provide a comprehensive evaluation of LCLMs.\n\nHELMET的设计目标是提供多样化的下游任务覆盖，可控的长度和复杂度，以及可靠的评估，支持基础模型和指令微调模型。\nHELMET is designed with diverse downstream task coverage, controllable length/complexity, and reliable evaluation for both base and instruction-tuned models.\n\n我们实验评估了8K到128K tokens的输入长度，HELMET可扩展到更长文本。\nIn our experiments, we evaluate on input length from 8K to 128K tokens, and HELMET is easily extended.\n\n## HELMET基准测试集：学术摘要\n\nHELMET includes diverse tasks such as retrieval-augmented generation, citation, and summarization.\nHELMET包含多种任务，例如检索增强生成、引文生成和摘要生成。\n\nWe select datasets with naturally long contexts for real-world applications.\n我们选择具有自然长上下文的数据集，以适应真实世界的应用。\n\nThese datasets are complemented with model-based evaluations and human studies for reliable evaluation.\n这些数据集辅以基于模型的评估和人工评估，以确保评估的可靠性。\n\nInput length is controlled by manipulating retrieved passages (RAG, Cite, Re-rank), demonstrations (ICL), or document length (LongQA, Summ).\n输入长度可以通过操纵检索到的段落（RAG、Cite、Re-rank）、演示（ICL）或文档长度（LongQA、Summ）来控制。\n\nLongQA and Summ use datasets with >100K token natural documents.\nLongQA和Summ使用包含超过10万token的自然文档的数据集。\n\nWe employ model-based evaluations, showing better distinguishability compared to n-gram metrics (ROUGE). *Figure 3*.\n我们采用基于模型的评估，相比于基于n-gram的指标（ROUGE），显示出更好的区分度。*图3*。\n\nHuman studies show high agreement with our evaluation metrics.\n人工评估显示出与我们的评估指标高度一致。\n\nWe support base models with in-context learning, improving their performance on our tasks.\n我们通过上下文学习支持基准模型，提高它们在任务上的表现。\n\n**摘要（Abstract）**\n\n长文本语言模型（LCLMs）在真实世界应用中仍有很长的路要走。\n*LCLMs still have a long way to go on real-world applications.*\n\n我们的实验和分析包括59个LCLMs的全面集合。\n*Our experiments and analyses include a comprehensive set of 59 LCLMs.*\n\n据我们所知，这是对不同应用上长文本模型最彻底和受控的比较。\n*To our knowledge, this is the most thorough and controlled comparison of long-context models on diverse applications.*\n\n评估长文本能力需要多样化的评估方式。\n*Diverse evaluation is needed for assessing long-context abilities.*\n\n长文本基准通常是针对特定应用构建的，限制了LCLMs在更广泛上下文中的理解。\n*Long-context benchmarks are often constructed with specific applications in mind, which limits the understanding of LCLMs in a broader context.*\n\n我们发现不同类别之间的表现并不总是相互关联（如图4）。\n*We examine model performance over a wide range of real tasks and find that different categories do not always correlate with each other (Figure 4).*\n\n模型性能随着长度的增加和任务复杂性的提高而下降。\n*Models degrade with increasing lengths and task complexity.*\n\n我们展示了前沿专有模型以及一些开源模型在HELMET上的结果（如图5）。\n*We present the results of the frontier proprietary models as well as a few open-source models on HELMET (Figure 5).*\n\n我们观察到开源模型在复杂任务上落后于闭源模型。\n*First, we observe that open-source models lag behind closed-source models on complex tasks.*\n\n**Abstract:**\n\n我们介绍HELMET，一个用于评估长上下文模型性能的综合框架。\nWe introduce HELMET, a comprehensive framework for evaluating the performance of long-context models.\n\nHELMET揭示了长上下文长度下性能退化的问题，例如在重排序任务中。\nHELMET reveals performance degradation with increasing context lengths, such as in re-ranking tasks.\n\n性能退化程度取决于任务类型。\nPerformance degradation is category-dependent.\n\n即使是GPT-4o和Gemini等最先进的模型也存在显著的性能下降。\nEven state-of-the-art models like GPT-4o and Gemini experience a significant performance decrease.\n\n不同任务类别没有明确的“最佳”模型，需要跨不同维度进行评估。\nThere is no clear winner across all categories, thereby calling for evaluation across different axes.\n\nHELMET易于使用，支持多种模型加载方式，包括`transformers`、TGI、Inference Endpoints、`vllm` 以及模型提供商的API。\nHELMET is easy to use and supports various model loading methods, including `transformers`, TGI, Inference Endpoints, `vllm`, and model provider APIs.\n\n通过简单的命令行 `python eval.py --config configs/rag.yaml --model_name_or_path <model_name>` 即可运行评估。\nEvaluations can be run with a simple command: `python eval.py --config configs/rag.yaml --model_name_or_path <model_name>`.\n\n**Abstract:**\n\n本文档概述了使用多种推理服务进行评估的方法。\nThis document outlines methods for evaluation using various inference services.\n\n包括通过 Text Generation Inference (TGI) (需要 `\"tgi:\"` 前缀 and `use_tgi_serving: true`)、HuggingFace Inference Endpoints、VLLM (`use_vllm_serving: true`)以及 OpenAI、Anthropic、Google 和 TogetherAI 等模型提供商的 API 进行模型评估。\nModel evaluation is possible via Text Generation Inference (TGI) (requiring `\"tgi:\"` prefix and `use_tgi_serving: true`), HuggingFace Inference Endpoints, VLLM (`use_vllm_serving: true`), and Model Provider APIs (OpenAI, Anthropic, Google, TogetherAI).\n\n通过设置 `LLM_ENPOINT` 和 API 密钥，并运行 `python eval.py --config configs/config.yaml --endpoint_url $LLM_ENDPOINT [--api_key $API_KEY]` 指令来启动评测。\nEvaluation is initiated by setting `LLM_ENPOINT` and API keys, then running `python eval.py --config configs/config.yaml --endpoint_url $LLM_ENDPOINT [--api_key $API_KEY]`.\n\n推荐使用 Recall 和 RAG 任务加速模型开发 (`python eval.py --config configs/rag.yaml --model_name_or_path <model_name>`).\nRecall and RAG tasks are recommended for faster model development (`python eval.py --config configs/rag.yaml --model_name_or_path <model_name>`).\n\n**Abstract**\n\n长文本语言模型（LCLMs）的评估，尤其是在长上下文情况下，鉴于其计算和内存成本，极具挑战性。\nEvaluating Long-Context Language Models (LCLMs), especially at long contexts, is challenging given their computational and memory costs.\n\n例如，在70B模型上运行所有长度的HELMET需要一个具有8 * 80GB GPU的节点，耗费数百GPU小时，成本高昂。\nFor example, running HELMET at all lengths on a 70B model requires a node with 8 * 80GB GPUs for hundreds of GPU hours, which can be costly.\n\n通过在HELMET上进行评估，研究人员只需参考我们的结果，即可直接将他们的模型与现有的59个不同大小和架构的模型进行比较。\nBy evaluating on HELMET, researchers can directly compare their models to existing ones simply by referencing our results, which cover 59 models of different sizes and architectures.\n\n我们最近发布了LongProc，一个用于评估LCLMs在长文本生成（long-form generation）和遵循程序（following procedures）方面的基准。\nWe recently released LongProc, a benchmark for evaluating LCLMs on long-form generation and following procedures.\n\nLongProc侧重于更长的输出，高达8K tokens，而不仅仅是摘要任务的1K tokens输出。\nLongProc focuses on even longer outputs, up to 8K tokens, unlike summarization tasks with up to 1K tokens outputs.\n\n我们正在努力将LongProc整合到HELMET的评估套件中，希望这将为LCLMs在长文本任务上提供更全面的评估。\nWe are working on integrating LongProc into HELMET's evaluation suite, and we hope that this will provide a more comprehensive evaluation of LCLMs on long-form tasks.\n\n```\n**中英对照摘要（Abstract - Bilingual Abstract）**\n\n本文概述了近期关于大型语言模型(LLM) 的讨论，重点关注Google Gemma 3和ModernBERT。\nThis abstract outlines recent discussions on Large Language Models (LLMs), highlighting Google's Gemma 3 and ModernBERT.\n\nGemma 3被描述为多模态、多语言、长上下文的开源LLM。\nGemma 3 is described as a multimodal, multilingual, long-context open-source LLM.\n\nModernBERT 则被视为BERT的替代方案。\nModernBERT is presented as a replacement for BERT.\n\n评论区提到了一个评估长上下文的新基准：NoLiMa，相关论文链接为https://github.com/adobe-research/NoLiMa。\nA comment mentions a new benchmark for evaluating long contexts: NoLiMa, with the related paper available at https://github.com/adobe-research/NoLiMa.\n\n该基准超越了字面匹配进行长上下文评估。\nThis benchmark goes beyond literal matching for long-context evaluation.\n\n总体而言，讨论围绕着LLM的最新进展和评估方法展开。\nOverall, the discussion revolves around the latest advancements and evaluation methods in LLMs.\n```"
}