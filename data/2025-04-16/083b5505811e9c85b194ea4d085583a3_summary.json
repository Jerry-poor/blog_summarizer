{
  "title": "Cohere on Hugging Face Inference Providers 🔥",
  "link": "https://huggingface.co/blog/inference-providers-cohere",
  "published": "2025-04-16",
  "summary": "## Summary (摘要)\n\n**English:**\n\nThis article announces that Cohere is now a supported Inference Provider on the Hugging Face Hub. Cohere is committed to providing secure AI solutions for enterprise use-cases, including Generative AI, Embeddings, and Ranking models. Cohere Labs supports fundamental research. Several Cohere models are now available for serverless inference, including:\n\n*   `CohereLabs/c4ai-command-r-v01`\n*   `CohereLabs/c4ai-command-r-plus`\n*   `CohereLabs/c4ai-command-r-08-2024`\n*   `CohereLabs/c4ai-command-r7b-12-2024`\n*   `CohereLabs/c4ai-command-a-03-2025`\n*   `CohereLabs/aya-expanse-8b`\n*   `CohereLabs/aya-expanse-32b`\n*   `CohereLabs/aya-vision-8b`\n*   `CohereLabs/aya-vision-32b`\n\nSpecifically, `CohereLabs/c4ai-command-a-03-2025` is highlighted for its suitability for enterprises requiring fast, secure, and high-quality AI, featuring a 256k context length.\n\n**中文:**\n\n这篇文章宣布 Cohere 现在是 Hugging Face Hub 上支持的 Inference Provider (推理提供商)。Cohere 致力于为企业用例提供安全的 AI 解决方案，包括 Generative AI (生成式AI)、Embeddings (嵌入) 和 Ranking (排序) 模型。 Cohere Labs 支持基础研究。现在可以通过 Cohere 和 Inference Providers 进行以下 Cohere 模型的 Serverless Inference (无服务器推理)：\n\n*   `CohereLabs/c4ai-command-r-v01`\n*   `CohereLabs/c4ai-command-r-plus`\n*   `CohereLabs/c4ai-command-r-08-2024`\n*   `CohereLabs/c4ai-command-r7b-12-2024`\n*   `CohereLabs/c4ai-command-a-03-2025`\n*   `CohereLabs/aya-expanse-8b`\n*   `CohereLabs/aya-expanse-32b`\n*   `CohereLabs/aya-vision-8b`\n*   `CohereLabs/aya-vision-32b`\n\n特别提到 `CohereLabs/c4ai-command-a-03-2025`，它适用于需要快速、安全和高质量 AI 的企业，并具有 256k context length (上下文长度)。\n\n## Summary in English and Chinese:\n\n**English:**\n\nThis text highlights Cohere's advanced Language Model offerings, emphasizing their multilingual capabilities and advanced features.  Key models mentioned include:\n\n*   **aya-expanse-32b:**  Focuses on state-of-the-art multilingual support (23 languages), leveraging recent research in multilingual pre-training. Features a 128K context length.\n*   **c4ai-command-r7b-12-2024:**  Ideal for low-cost/low-latency use cases.  Offers 128K context length, multilingual support (23 languages), citation-verified **Retrieval-Augmented Generation (RAG)**, reasoning, tool use, and agentic behavior. An open-weight model with strong performance.\n*   **aya-vision-32b:**  A 32-billion parameter vision-language model optimized for tasks such as **OCR**, captioning, visual reasoning, summarization, question answering, and code. Supports 23 languages.\n\nThe text also describes how to use Cohere models on the Hugging Face Hub via the website UI and client SDKs (specifically Python with `huggingface_hub`).  It provides links to the Cohere documentation page and a Colab notebook for examples.\n\n**Chinese:**\n\n该文本重点介绍了Cohere先进的语言模型产品，强调其多语言能力和高级功能。提到的关键模型包括：\n\n*   **aya-expanse-32b:** 专注于最先进的多语言支持（23种语言），利用多语言预训练的最新研究。具有128K的上下文长度。\n*   **c4ai-command-r7b-12-2024:**  适用于低成本/低延迟用例。提供 128K 的上下文长度，多语言支持（23 种语言），引用验证的**检索增强生成 (RAG)**，推理，工具使用和 Agentic 行为。一个具有强大性能的开放权重模型。\n*   **aya-vision-32b:** 一个320亿参数的视觉语言模型，针对诸如 **OCR**、字幕、视觉推理、摘要、问答和代码等任务进行了优化。支持23种语言。\n\n该文本还描述了如何在 Hugging Face Hub 上通过网站 UI 和客户端 SDK（特别是使用 `huggingface_hub` 的 Python）使用 Cohere 模型。 它提供了指向 Cohere 文档页面和 Colab notebook 的链接，以供参考示例。\n\n## Summary (摘要):\n\nThis document describes how to use Cohere as an inference provider via the Hugging Face Hub. It details the necessary setup, including installing `huggingface_hub>=0.30.0` and using the `InferenceClient` with the `provider=\"cohere\"` parameter and your Cohere API key.\n\n**Key points:**\n\n*   **Setup:** Install `huggingface_hub>=0.30.0`.\n*   **`InferenceClient`:** Instantiate with `provider=\"cohere\"` and your API key:\n\n    ```python\n    from huggingface_hub import InferenceClient\n\n    client = InferenceClient(\n        provider=\"cohere\",\n        api_key=\"xxxxxxxxxxxxxxxxxxxxxxxx\",\n    )\n    ```\n\n*   **Chat Completion:** Use `client.chat.completions.create()` to generate responses.  Example usage with `CohereLabs/c4ai-command-r7b-12-2024`:\n\n    ```python\n    completion = client.chat.completions.create(\n        model=\"CohereLabs/c4ai-command-r7b-12-2024\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=512,\n    )\n    ```\n\n*   **Multimodal Support (Aya Vision):** Shows how to send base64 encoded images to Cohere's Aya Vision model:\n\n    ```python\n    image_path = \"img.jpg\"\n    with open(image_path, \"rb\") as f:\n        base64_image = base64.b64encode(f.read()).decode(\"utf-8\")\n    image_url = f\"data:image/jpeg;base64,{base64_image}\"\n\n    messages = [\n            {\n    \"role\": \"user\",\n    \"content\": [\n                    {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n                ]\n            }\n    ]\n\n    completion = client.chat.completions.create(\n        model=\"CohereLabs/aya-vision-32b\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=512,\n    )\n    ```\n*   **JavaScript Example:** Demonstrates using `@huggingface/inference` to call Cohere chat completion endpoints from JavaScript.\n```javascript\nimport { HfInference } from \"@huggingface/inference\";\nconst client = new HfInference(\"xxxxxxxxxxxxxxxxxxxxxxxx\");\nconst chatCompletion = await client.chatCompletion({\n    model: \"CohereLabs/c4ai-command-a-03-2025\",\n    messages: [{ role: \"user\", content: \"How to mak\"}],\n});\n```\n\n---\n\n## 中文摘要:\n\n本文档描述了如何通过 Hugging Face Hub 将 Cohere 用作推理提供商。 它详细介绍了必要的设置，包括安装 `huggingface_hub>=0.30.0` 和使用带有 `provider=\"cohere\"` 参数和您的 Cohere API 密钥的 `InferenceClient`。\n\n**关键点：**\n\n*   **设置：** 安装 `huggingface_hub>=0.30.0`。\n*   **`InferenceClient`：** 使用 `provider=\"cohere\"` 和您的 API 密钥进行实例化：\n\n    ```python\n    from huggingface_hub import InferenceClient\n\n    client = InferenceClient(\n        provider=\"cohere\",\n        api_key=\"xxxxxxxxxxxxxxxxxxxxxxxx\",\n    )\n    ```\n\n*   **聊天补全：** 使用 `client.chat.completions.create()` 生成响应。 使用 `CohereLabs/c4ai-command-r7b-12-2024` 的示例用法：\n\n    ```python\n    completion = client.chat.completions.create(\n        model=\"CohereLabs/c4ai-command-r7b-12-2024\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=512,\n    )\n    ```\n\n*   **多模态支持 (Aya Vision):** 展示了如何将 base64 编码的图像发送到 Cohere 的 Aya Vision 模型：\n\n    ```python\n    image_path = \"img.jpg\"\n    with open(image_path, \"rb\") as f:\n        base64_image = base64.b64encode(f.read()).decode(\"utf-8\")\n    image_url = f\"data:image/jpeg;base64,{base64_image}\"\n\n    messages = [\n            {\n    \"role\": \"user\",\n    \"content\": [\n                    {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n                ]\n            }\n    ]\n\n    completion = client.chat.completions.create(\n        model=\"CohereLabs/aya-vision-32b\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=512,\n    )\n    ```\n\n*  **JavaScript 示例：**演示了使用 `@huggingface/inference` 从 JavaScript 调用 Cohere 聊天补全端点。\n```javascript\nimport { HfInference } from \"@huggingface/inference\";\nconst client = new HfInference(\"xxxxxxxxxxxxxxxxxxxxxxxx\");\nconst chatCompletion = await client.chatCompletion({\n    model: \"CohereLabs/c4ai-command-a-03-2025\",\n    messages: [{ role: \"user\", content: \"How to mak\"}],\n});\n```\n\n## 摘要 (Summary)\n\n**中文:**\n\n本文展示了如何通过 OpenAI 客户端库，使用 Cohere 作为推理提供者来调用 Command R7B 模型。文章提供了使用 `OpenAI` 客户端进行聊天补全的代码示例，包括设置 `base_url` 和 `api_key`，以及传递包含用户消息 `messages` 的列表。还介绍了 Cohere 模型的工具使用功能 (Tool Use)，并展示了如何定义工具（例如 `get_flight_info`，用于获取两地之间的航班信息）以及如何在 `tool_calls` 中传递消息给推理客户端，以便模型在需要时使用这些工具。 工具的定义包括 `type`，`function` 和 `parameters`，其中参数包括 `loc_origin` 和 `loc_destination`，用于指定起飞地和目的地机场。\n\n**English:**\n\nThis document demonstrates how to call the Command R7B model using Cohere as the inference provider via the OpenAI client library. It provides a code example of using the `OpenAI` client for chat completions, including setting the `base_url` and `api_key`, and passing a list of `messages` containing the user's message. It also introduces the Tool Use feature of Cohere models and shows how to define tools (e.g., `get_flight_info`, which retrieves flight information between two locations) and how to pass messages with `tool_calls` to the inference client so that the model can use these tools when relevant. The tool definition includes `type`, `function`, and `parameters`, where the parameters include `loc_origin` and `loc_destination` for specifying the departure and destination airports.\n\n好的，这是对你提供的内容的摘要，包含中英双语，并保留了公式与术语：\n\n**摘要：**\n\nThis text demonstrates how to use the `huggingface_hub` library and `InferenceClient` to interact with a large language model (LLM) from Cohere. It provides a code snippet that uses the `client.chat.completions.create` method to generate a response based on a series of messages and tools. The example shows a multi-turn conversation where a user asks about flight information, the assistant uses a tool (a function call with `name` \"get_flight_info\" and `arguments` `'{ \"loc_destination\": \"Seattle\", \"loc_origin\": \"Miami\" }'`) to retrieve the information, and the tool returns the flight details.  The code sets parameters like `model` (\"CohereLabs/c4ai-command-r7b-12-2024\"), `messages`, `tools`, `temperature` (0.7), and `max_tokens` (512). The document also outlines billing information for both direct requests (using a Cohere key) and routed requests (authenticating via the Hugging Face Hub).  PRO users get $2 worth of Inference credits per month.\n\n**中文摘要：**\n\n这段文字展示了如何使用 `huggingface_hub` 库和 `InferenceClient` 来与 Cohere 的大型语言模型 (LLM) 交互。它提供了一段代码，使用 `client.chat.completions.create` 方法基于一系列的消息和工具来生成回复。示例展示了一个多轮对话，其中用户询问航班信息，助手使用一个工具（一个函数调用， `name` 为 \"get_flight_info\"， `arguments` 为 `'{ \"loc_destination\": \"Seattle\", \"loc_origin\": \"Miami\" }'`）来检索信息，然后工具返回航班详情。代码设置了诸如 `model` (\"CohereLabs/c4ai-command-r7b-12-2024\")、`messages`、`tools`、`temperature` (0.7) 和 `max_tokens` (512) 等参数。该文档还概述了直接请求（使用 Cohere 密钥）和路由请求（通过 Hugging Face Hub 验证）的计费信息。PRO 用户每月可获得价值 2 美元的推理积分。\n\n关键术语：`huggingface_hub`, `InferenceClient`, `client.chat.completions.create`, `messages`, `tools`, `temperature`, `max_tokens`, `model`, `function call`, `name`, `arguments`.\n\n**摘要 (Summary):**\n\nThe user \"borgr\" inquired about the possibility of exporting or retrieving user conversations from the UI or via API, potentially for purposes such as data labeling, studying user needs (\"what people lack\"), and understanding human reactions to Large Language Model (LM) behavior. The article author, \"merve,\" responded that such an option is not currently available but will be considered. They suggested using the providers programmatically and storing the data independently in the meantime.\n\n**英文摘要 (English Summary):**\n\nThe user \"borgr\" asked if there was a way to export user conversations from the UI or retrieve them via an API request. Their use cases included data labeling, studying user needs (\"what people lack\"), and understanding human reactions to Large Language Model (LM) behavior. The article author, \"merve,\" replied that this functionality is not yet available but will be considered. They suggested using the providers programmatically and storing the data independently for now."
}