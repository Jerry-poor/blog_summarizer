{
  "title": "Cohere on Hugging Face Inference Providers ğŸ”¥",
  "link": "https://huggingface.co/blog/inference-providers-cohere",
  "published": "2025-04-16",
  "summary": "## Summary (æ‘˜è¦)\n\n**English:**\n\nThis article announces that Cohere is now a supported Inference Provider on the Hugging Face Hub. Cohere is committed to providing secure AI solutions for enterprise use-cases, including Generative AI, Embeddings, and Ranking models. Cohere Labs supports fundamental research. Several Cohere models are now available for serverless inference, including:\n\n*   `CohereLabs/c4ai-command-r-v01`\n*   `CohereLabs/c4ai-command-r-plus`\n*   `CohereLabs/c4ai-command-r-08-2024`\n*   `CohereLabs/c4ai-command-r7b-12-2024`\n*   `CohereLabs/c4ai-command-a-03-2025`\n*   `CohereLabs/aya-expanse-8b`\n*   `CohereLabs/aya-expanse-32b`\n*   `CohereLabs/aya-vision-8b`\n*   `CohereLabs/aya-vision-32b`\n\nSpecifically, `CohereLabs/c4ai-command-a-03-2025` is highlighted for its suitability for enterprises requiring fast, secure, and high-quality AI, featuring a 256k context length.\n\n**ä¸­æ–‡:**\n\nè¿™ç¯‡æ–‡ç« å®£å¸ƒ Cohere ç°åœ¨æ˜¯ Hugging Face Hub ä¸Šæ”¯æŒçš„ Inference Provider (æ¨ç†æä¾›å•†)ã€‚Cohere è‡´åŠ›äºä¸ºä¼ä¸šç”¨ä¾‹æä¾›å®‰å…¨çš„ AI è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ Generative AI (ç”Ÿæˆå¼AI)ã€Embeddings (åµŒå…¥) å’Œ Ranking (æ’åº) æ¨¡å‹ã€‚ Cohere Labs æ”¯æŒåŸºç¡€ç ”ç©¶ã€‚ç°åœ¨å¯ä»¥é€šè¿‡ Cohere å’Œ Inference Providers è¿›è¡Œä»¥ä¸‹ Cohere æ¨¡å‹çš„ Serverless Inference (æ— æœåŠ¡å™¨æ¨ç†)ï¼š\n\n*   `CohereLabs/c4ai-command-r-v01`\n*   `CohereLabs/c4ai-command-r-plus`\n*   `CohereLabs/c4ai-command-r-08-2024`\n*   `CohereLabs/c4ai-command-r7b-12-2024`\n*   `CohereLabs/c4ai-command-a-03-2025`\n*   `CohereLabs/aya-expanse-8b`\n*   `CohereLabs/aya-expanse-32b`\n*   `CohereLabs/aya-vision-8b`\n*   `CohereLabs/aya-vision-32b`\n\nç‰¹åˆ«æåˆ° `CohereLabs/c4ai-command-a-03-2025`ï¼Œå®ƒé€‚ç”¨äºéœ€è¦å¿«é€Ÿã€å®‰å…¨å’Œé«˜è´¨é‡ AI çš„ä¼ä¸šï¼Œå¹¶å…·æœ‰ 256k context length (ä¸Šä¸‹æ–‡é•¿åº¦)ã€‚\n\n## Summary in English and Chinese:\n\n**English:**\n\nThis text highlights Cohere's advanced Language Model offerings, emphasizing their multilingual capabilities and advanced features.  Key models mentioned include:\n\n*   **aya-expanse-32b:**  Focuses on state-of-the-art multilingual support (23 languages), leveraging recent research in multilingual pre-training. Features a 128K context length.\n*   **c4ai-command-r7b-12-2024:**  Ideal for low-cost/low-latency use cases.  Offers 128K context length, multilingual support (23 languages), citation-verified **Retrieval-Augmented Generation (RAG)**, reasoning, tool use, and agentic behavior. An open-weight model with strong performance.\n*   **aya-vision-32b:**  A 32-billion parameter vision-language model optimized for tasks such as **OCR**, captioning, visual reasoning, summarization, question answering, and code. Supports 23 languages.\n\nThe text also describes how to use Cohere models on the Hugging Face Hub via the website UI and client SDKs (specifically Python with `huggingface_hub`).  It provides links to the Cohere documentation page and a Colab notebook for examples.\n\n**Chinese:**\n\nè¯¥æ–‡æœ¬é‡ç‚¹ä»‹ç»äº†Cohereå…ˆè¿›çš„è¯­è¨€æ¨¡å‹äº§å“ï¼Œå¼ºè°ƒå…¶å¤šè¯­è¨€èƒ½åŠ›å’Œé«˜çº§åŠŸèƒ½ã€‚æåˆ°çš„å…³é”®æ¨¡å‹åŒ…æ‹¬ï¼š\n\n*   **aya-expanse-32b:** ä¸“æ³¨äºæœ€å…ˆè¿›çš„å¤šè¯­è¨€æ”¯æŒï¼ˆ23ç§è¯­è¨€ï¼‰ï¼Œåˆ©ç”¨å¤šè¯­è¨€é¢„è®­ç»ƒçš„æœ€æ–°ç ”ç©¶ã€‚å…·æœ‰128Kçš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚\n*   **c4ai-command-r7b-12-2024:**  é€‚ç”¨äºä½æˆæœ¬/ä½å»¶è¿Ÿç”¨ä¾‹ã€‚æä¾› 128K çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¤šè¯­è¨€æ”¯æŒï¼ˆ23 ç§è¯­è¨€ï¼‰ï¼Œå¼•ç”¨éªŒè¯çš„**æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)**ï¼Œæ¨ç†ï¼Œå·¥å…·ä½¿ç”¨å’Œ Agentic è¡Œä¸ºã€‚ä¸€ä¸ªå…·æœ‰å¼ºå¤§æ€§èƒ½çš„å¼€æ”¾æƒé‡æ¨¡å‹ã€‚\n*   **aya-vision-32b:** ä¸€ä¸ª320äº¿å‚æ•°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé’ˆå¯¹è¯¸å¦‚ **OCR**ã€å­—å¹•ã€è§†è§‰æ¨ç†ã€æ‘˜è¦ã€é—®ç­”å’Œä»£ç ç­‰ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚æ”¯æŒ23ç§è¯­è¨€ã€‚\n\nè¯¥æ–‡æœ¬è¿˜æè¿°äº†å¦‚ä½•åœ¨ Hugging Face Hub ä¸Šé€šè¿‡ç½‘ç«™ UI å’Œå®¢æˆ·ç«¯ SDKï¼ˆç‰¹åˆ«æ˜¯ä½¿ç”¨ `huggingface_hub` çš„ Pythonï¼‰ä½¿ç”¨ Cohere æ¨¡å‹ã€‚ å®ƒæä¾›äº†æŒ‡å‘ Cohere æ–‡æ¡£é¡µé¢å’Œ Colab notebook çš„é“¾æ¥ï¼Œä»¥ä¾›å‚è€ƒç¤ºä¾‹ã€‚\n\n## Summary (æ‘˜è¦):\n\nThis document describes how to use Cohere as an inference provider via the Hugging Face Hub. It details the necessary setup, including installing `huggingface_hub>=0.30.0` and using the `InferenceClient` with the `provider=\"cohere\"` parameter and your Cohere API key.\n\n**Key points:**\n\n*   **Setup:** Install `huggingface_hub>=0.30.0`.\n*   **`InferenceClient`:** Instantiate with `provider=\"cohere\"` and your API key:\n\n    ```python\n    from huggingface_hub import InferenceClient\n\n    client = InferenceClient(\n        provider=\"cohere\",\n        api_key=\"xxxxxxxxxxxxxxxxxxxxxxxx\",\n    )\n    ```\n\n*   **Chat Completion:** Use `client.chat.completions.create()` to generate responses.  Example usage with `CohereLabs/c4ai-command-r7b-12-2024`:\n\n    ```python\n    completion = client.chat.completions.create(\n        model=\"CohereLabs/c4ai-command-r7b-12-2024\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=512,\n    )\n    ```\n\n*   **Multimodal Support (Aya Vision):** Shows how to send base64 encoded images to Cohere's Aya Vision model:\n\n    ```python\n    image_path = \"img.jpg\"\n    with open(image_path, \"rb\") as f:\n        base64_image = base64.b64encode(f.read()).decode(\"utf-8\")\n    image_url = f\"data:image/jpeg;base64,{base64_image}\"\n\n    messages = [\n            {\n    \"role\": \"user\",\n    \"content\": [\n                    {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n                ]\n            }\n    ]\n\n    completion = client.chat.completions.create(\n        model=\"CohereLabs/aya-vision-32b\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=512,\n    )\n    ```\n*   **JavaScript Example:** Demonstrates using `@huggingface/inference` to call Cohere chat completion endpoints from JavaScript.\n```javascript\nimport { HfInference } from \"@huggingface/inference\";\nconst client = new HfInference(\"xxxxxxxxxxxxxxxxxxxxxxxx\");\nconst chatCompletion = await client.chatCompletion({\n    model: \"CohereLabs/c4ai-command-a-03-2025\",\n    messages: [{ role: \"user\", content: \"How to mak\"}],\n});\n```\n\n---\n\n## ä¸­æ–‡æ‘˜è¦:\n\næœ¬æ–‡æ¡£æè¿°äº†å¦‚ä½•é€šè¿‡ Hugging Face Hub å°† Cohere ç”¨ä½œæ¨ç†æä¾›å•†ã€‚ å®ƒè¯¦ç»†ä»‹ç»äº†å¿…è¦çš„è®¾ç½®ï¼ŒåŒ…æ‹¬å®‰è£… `huggingface_hub>=0.30.0` å’Œä½¿ç”¨å¸¦æœ‰ `provider=\"cohere\"` å‚æ•°å’Œæ‚¨çš„ Cohere API å¯†é’¥çš„ `InferenceClient`ã€‚\n\n**å…³é”®ç‚¹ï¼š**\n\n*   **è®¾ç½®ï¼š** å®‰è£… `huggingface_hub>=0.30.0`ã€‚\n*   **`InferenceClient`ï¼š** ä½¿ç”¨ `provider=\"cohere\"` å’Œæ‚¨çš„ API å¯†é’¥è¿›è¡Œå®ä¾‹åŒ–ï¼š\n\n    ```python\n    from huggingface_hub import InferenceClient\n\n    client = InferenceClient(\n        provider=\"cohere\",\n        api_key=\"xxxxxxxxxxxxxxxxxxxxxxxx\",\n    )\n    ```\n\n*   **èŠå¤©è¡¥å…¨ï¼š** ä½¿ç”¨ `client.chat.completions.create()` ç”Ÿæˆå“åº”ã€‚ ä½¿ç”¨ `CohereLabs/c4ai-command-r7b-12-2024` çš„ç¤ºä¾‹ç”¨æ³•ï¼š\n\n    ```python\n    completion = client.chat.completions.create(\n        model=\"CohereLabs/c4ai-command-r7b-12-2024\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=512,\n    )\n    ```\n\n*   **å¤šæ¨¡æ€æ”¯æŒ (Aya Vision):** å±•ç¤ºäº†å¦‚ä½•å°† base64 ç¼–ç çš„å›¾åƒå‘é€åˆ° Cohere çš„ Aya Vision æ¨¡å‹ï¼š\n\n    ```python\n    image_path = \"img.jpg\"\n    with open(image_path, \"rb\") as f:\n        base64_image = base64.b64encode(f.read()).decode(\"utf-8\")\n    image_url = f\"data:image/jpeg;base64,{base64_image}\"\n\n    messages = [\n            {\n    \"role\": \"user\",\n    \"content\": [\n                    {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n                ]\n            }\n    ]\n\n    completion = client.chat.completions.create(\n        model=\"CohereLabs/aya-vision-32b\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=512,\n    )\n    ```\n\n*  **JavaScript ç¤ºä¾‹ï¼š**æ¼”ç¤ºäº†ä½¿ç”¨ `@huggingface/inference` ä» JavaScript è°ƒç”¨ Cohere èŠå¤©è¡¥å…¨ç«¯ç‚¹ã€‚\n```javascript\nimport { HfInference } from \"@huggingface/inference\";\nconst client = new HfInference(\"xxxxxxxxxxxxxxxxxxxxxxxx\");\nconst chatCompletion = await client.chatCompletion({\n    model: \"CohereLabs/c4ai-command-a-03-2025\",\n    messages: [{ role: \"user\", content: \"How to mak\"}],\n});\n```\n\n## æ‘˜è¦ (Summary)\n\n**ä¸­æ–‡:**\n\næœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ OpenAI å®¢æˆ·ç«¯åº“ï¼Œä½¿ç”¨ Cohere ä½œä¸ºæ¨ç†æä¾›è€…æ¥è°ƒç”¨ Command R7B æ¨¡å‹ã€‚æ–‡ç« æä¾›äº†ä½¿ç”¨ `OpenAI` å®¢æˆ·ç«¯è¿›è¡ŒèŠå¤©è¡¥å…¨çš„ä»£ç ç¤ºä¾‹ï¼ŒåŒ…æ‹¬è®¾ç½® `base_url` å’Œ `api_key`ï¼Œä»¥åŠä¼ é€’åŒ…å«ç”¨æˆ·æ¶ˆæ¯ `messages` çš„åˆ—è¡¨ã€‚è¿˜ä»‹ç»äº† Cohere æ¨¡å‹çš„å·¥å…·ä½¿ç”¨åŠŸèƒ½ (Tool Use)ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•å®šä¹‰å·¥å…·ï¼ˆä¾‹å¦‚ `get_flight_info`ï¼Œç”¨äºè·å–ä¸¤åœ°ä¹‹é—´çš„èˆªç­ä¿¡æ¯ï¼‰ä»¥åŠå¦‚ä½•åœ¨ `tool_calls` ä¸­ä¼ é€’æ¶ˆæ¯ç»™æ¨ç†å®¢æˆ·ç«¯ï¼Œä»¥ä¾¿æ¨¡å‹åœ¨éœ€è¦æ—¶ä½¿ç”¨è¿™äº›å·¥å…·ã€‚ å·¥å…·çš„å®šä¹‰åŒ…æ‹¬ `type`ï¼Œ`function` å’Œ `parameters`ï¼Œå…¶ä¸­å‚æ•°åŒ…æ‹¬ `loc_origin` å’Œ `loc_destination`ï¼Œç”¨äºæŒ‡å®šèµ·é£åœ°å’Œç›®çš„åœ°æœºåœºã€‚\n\n**English:**\n\nThis document demonstrates how to call the Command R7B model using Cohere as the inference provider via the OpenAI client library. It provides a code example of using the `OpenAI` client for chat completions, including setting the `base_url` and `api_key`, and passing a list of `messages` containing the user's message. It also introduces the Tool Use feature of Cohere models and shows how to define tools (e.g., `get_flight_info`, which retrieves flight information between two locations) and how to pass messages with `tool_calls` to the inference client so that the model can use these tools when relevant. The tool definition includes `type`, `function`, and `parameters`, where the parameters include `loc_origin` and `loc_destination` for specifying the departure and destination airports.\n\nå¥½çš„ï¼Œè¿™æ˜¯å¯¹ä½ æä¾›çš„å†…å®¹çš„æ‘˜è¦ï¼ŒåŒ…å«ä¸­è‹±åŒè¯­ï¼Œå¹¶ä¿ç•™äº†å…¬å¼ä¸æœ¯è¯­ï¼š\n\n**æ‘˜è¦ï¼š**\n\nThis text demonstrates how to use the `huggingface_hub` library and `InferenceClient` to interact with a large language model (LLM) from Cohere. It provides a code snippet that uses the `client.chat.completions.create` method to generate a response based on a series of messages and tools. The example shows a multi-turn conversation where a user asks about flight information, the assistant uses a tool (a function call with `name` \"get_flight_info\" and `arguments` `'{ \"loc_destination\": \"Seattle\", \"loc_origin\": \"Miami\" }'`) to retrieve the information, and the tool returns the flight details.  The code sets parameters like `model` (\"CohereLabs/c4ai-command-r7b-12-2024\"), `messages`, `tools`, `temperature` (0.7), and `max_tokens` (512). The document also outlines billing information for both direct requests (using a Cohere key) and routed requests (authenticating via the Hugging Face Hub).  PRO users get $2 worth of Inference credits per month.\n\n**ä¸­æ–‡æ‘˜è¦ï¼š**\n\nè¿™æ®µæ–‡å­—å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `huggingface_hub` åº“å’Œ `InferenceClient` æ¥ä¸ Cohere çš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) äº¤äº’ã€‚å®ƒæä¾›äº†ä¸€æ®µä»£ç ï¼Œä½¿ç”¨ `client.chat.completions.create` æ–¹æ³•åŸºäºä¸€ç³»åˆ—çš„æ¶ˆæ¯å’Œå·¥å…·æ¥ç”Ÿæˆå›å¤ã€‚ç¤ºä¾‹å±•ç¤ºäº†ä¸€ä¸ªå¤šè½®å¯¹è¯ï¼Œå…¶ä¸­ç”¨æˆ·è¯¢é—®èˆªç­ä¿¡æ¯ï¼ŒåŠ©æ‰‹ä½¿ç”¨ä¸€ä¸ªå·¥å…·ï¼ˆä¸€ä¸ªå‡½æ•°è°ƒç”¨ï¼Œ `name` ä¸º \"get_flight_info\"ï¼Œ `arguments` ä¸º `'{ \"loc_destination\": \"Seattle\", \"loc_origin\": \"Miami\" }'`ï¼‰æ¥æ£€ç´¢ä¿¡æ¯ï¼Œç„¶åå·¥å…·è¿”å›èˆªç­è¯¦æƒ…ã€‚ä»£ç è®¾ç½®äº†è¯¸å¦‚ `model` (\"CohereLabs/c4ai-command-r7b-12-2024\")ã€`messages`ã€`tools`ã€`temperature` (0.7) å’Œ `max_tokens` (512) ç­‰å‚æ•°ã€‚è¯¥æ–‡æ¡£è¿˜æ¦‚è¿°äº†ç›´æ¥è¯·æ±‚ï¼ˆä½¿ç”¨ Cohere å¯†é’¥ï¼‰å’Œè·¯ç”±è¯·æ±‚ï¼ˆé€šè¿‡ Hugging Face Hub éªŒè¯ï¼‰çš„è®¡è´¹ä¿¡æ¯ã€‚PRO ç”¨æˆ·æ¯æœˆå¯è·å¾—ä»·å€¼ 2 ç¾å…ƒçš„æ¨ç†ç§¯åˆ†ã€‚\n\nå…³é”®æœ¯è¯­ï¼š`huggingface_hub`, `InferenceClient`, `client.chat.completions.create`, `messages`, `tools`, `temperature`, `max_tokens`, `model`, `function call`, `name`, `arguments`.\n\n**æ‘˜è¦ (Summary):**\n\nThe user \"borgr\" inquired about the possibility of exporting or retrieving user conversations from the UI or via API, potentially for purposes such as data labeling, studying user needs (\"what people lack\"), and understanding human reactions to Large Language Model (LM) behavior. The article author, \"merve,\" responded that such an option is not currently available but will be considered. They suggested using the providers programmatically and storing the data independently in the meantime.\n\n**è‹±æ–‡æ‘˜è¦ (English Summary):**\n\nThe user \"borgr\" asked if there was a way to export user conversations from the UI or retrieve them via an API request. Their use cases included data labeling, studying user needs (\"what people lack\"), and understanding human reactions to Large Language Model (LM) behavior. The article author, \"merve,\" replied that this functionality is not yet available but will be considered. They suggested using the providers programmatically and storing the data independently for now."
}