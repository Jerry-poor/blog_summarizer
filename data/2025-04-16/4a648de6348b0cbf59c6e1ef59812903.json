{
  "title": "Introducing HELMET",
  "link": "https://huggingface.co/blog/helmet",
  "published": "2025-04-16",
  "summary": "",
  "content": "Back to Articles\nIntroducing\nHELMET\n: Holistically Evaluating Long-context Language Models\nPublished\n\t\t\t\tApril 16, 2025\nUpdate on GitHub\nUpvote\n22\n+16\nhyen\nHoward Yen\nguest\ngaotianyu1350\nTianyu Gao\nguest\nhouminmin\nMinmin Hou\nIntel\nkding1\nKe Ding\nIntel\ndanf\nDaniel Fleischer\nIntel\nmoshew\nMoshe Wasserblat\nIntel\ncdq10131\nDanqi Chen\nguest\nEvaluating long-context language models is challenging but important\nExisting evaluations overly rely on synthetic tasks\nCrafting diverse, controllable, and reliable evaluation for LCLMs\nKey improvements over existing benchmarks\nLCLMs still have a long way to go on real-world tasks\nDiverse evaluation is needed for assessing long-context abilities\nModels degrade with increasing lengths and task complexity\nUsing HELMET for future developments\nHow to run HELMET\nFaster development\nQuick comparison with existing models\nLooking ahead\nAcknowledgements\nCitation\nContact:\nhyen@cs.princeton.edu\nPaper:\nhttps://arxiv.org/abs/2410.02694\nWebsite:\nhttps://princeton-nlp.github.io/HELMET\nCode & Data:\nhttps://github.com/princeton-nlp/HELMET\nSince we first released HELMET last October, there has been more development on long-context language models than ever before, and we are thrilled to see the adoption of HELMET by the community, such as\nMicrosoft's Phi-4\nand\nAI21's Jamba 1.6\n.\nAfter the initial release, we have added more models to our evaluation suite and conducted additional analyses. We are excited to share our new results and present HELMET at ICLR 2025!\nIn this blog, we will describe the construction of HELMET, our key findings, and how practitioners can use HELMET to differentiate between various LCLMs in future research and applications.\nFinally, we will conclude with a quickstart guide for using HELMET with HuggingFace.\nEvaluating long-context language models is challenging but important\nFrom summarizing numerous legal documents to learning new tasks on the fly, long-context language models (LCLMs) have immense potential to change the way we use and interact with language models.\nLanguage models have been limited by their context window, which is around 2K to 8K tokens (e.g.,\nChatGPT\n,\nLlama-2/3\n).\nRecently, model developers have been constantly increasing the context window of their models, with recent models like\nGPT-4o\n,\nClaude-3\n, and\nGemini-1.5\nsupporting context windows of up to millions of tokens.\nFigure 1: Existing benchmarks show counterintuitive trends, such as smaller models outperforming larger ones (e.g., Llama-3.1 8B > 70B).\nHowever, with longer context windows, previous natural language benchmarks (e.g.,\nScrolls\n) are no longer suitable for evaluating LCLMs.\nConsequently, perplexity and synthetic tasks (e.g., needle-in-a-haystack) emerged as the most popular evaluation metrics for recent LCLMs, but they often\ndo not reflect real-world performance\n.\nModel developers may also evaluate on other arbitrary datasets, which complicates model comparisons.\nFurthermore, existing benchmarks for LCLMs may show confusing and counterintuitive results, making it difficult to understand the strengths and weaknesses of different models (Figure 1).\nIn this work, we propose HELMET (How to Evaluate Long-Context Models Effectively and Thoroughly), a comprehensive benchmark for evaluating LCLMs that improves upon existing benchmarks in several ways—\ndiversity, controllability, and reliability\n.\nWe evaluate 59 recent LCLMs and find that it is crucial to evaluate models across diverse applications to understand their capabilities and frontier LCLMs are still limited on complex tasks.\nExisting evaluations overly rely on synthetic tasks\nWith the development of LCLMs across both industry and the open-source community, it is crucial to have a reliable method for evaluating and comparing these models. However, current models are\noften evaluated on different benchmarks\n(Table 1).\nTable 1: Model developers often evaluate on different sets of datasets.\n♭\n: Base models. NQA: NarrativeQA, Qspr: Qasper, QALT: QuALITY, SQALT: SQuALTY.\nA common practice for evaluating long-context language models is to use perplexity or synthetic tasks, such as needle-in-a-haystack (NIAH). \nHowever, recent works have shown that perplexity does not correlate well with downstream performance (\nFang et al., 2024\n). \nIn Figure 2, we show that synthetic tasks like NIAH do not correlate with real-world performance, but the more complex synthetic tasks achieve higher correlation with real-world tasks.\nFigure 2: Simple synthetic tasks, such as NIAH, do not correlate well with downstream tasks, such as summarization or generation with citations. More complex variants (e.g., RULER MV) achieve higher correlation.\nAmong the existing benchmarks with realistic applications, such as ZeroScrolls (\nShaman et al., 2023\n), LongBench (\nBai et al., 2024\n), and InfiniteBench (\nZhang et al., 2024\n), there are still crucial limitations:\nInsufficient coverage of downstream tasks: often focused on specific domains\nInadequate lengths for testing frontier LCLMs: older QA datasets are often limited to <32K tokens (e.g.,\nQASPER\n,\nQuALITY\n)\nUnreliable metrics: N-gram matching metrics like ROUGE are noisy—they do not correlate with human judgments (\nGoyal et al., 2023\n) and do not distinguish between models\nIncompatibility with base models: require instruction-tuning, which means they cannot be used for base model development\nThus, we propose HELMET to address these limitations and provide a comprehensive evaluation of LCLMs.\nCrafting diverse, controllable, and reliable evaluation for LCLMs\nWe design HELMET with the following desiderata:\nDiverse coverage of downstream tasks\nControllable length and complexity\nReliable evaluation for base and instruction-tuned models\nTable 2 shows an overview of the benchmark.\nIn our experiments, we evaluate on input length from 8K to 128K tokens, but HELMET can be easily extended to even longer context lengths.\nTable 2: Overview of HELMET datasets. SubEM: Substring Exact Match.\nKey improvements over existing benchmarks\nDiverse coverage\n: HELMET includes a diverse set of tasks, such as retrieval-augmented generation with real retrieval passages, generation with citations, and summarization. We carefully select datasets with naturally long contexts that reflect real-world applications. These datasets are complemented with reliable evaluation settings, such as model-based evaluations and human studies.\nControllable length and difficulty\n: An important dimension to consider when evaluating LCLMs is the input length, as longer inputs can provide more information while challenging the model's ability to process noisy contexts. In our tasks, we can control the input length by changing the number of retrieved passages (RAG, Cite, Re-rank), the number of demonstrations (ICL), or the length of the input document (LongQA, Summ). Although LongQA and Summ cannot be easily extended to longer contexts, we intentionally chose datasets with natural documents of length far greater than 100K tokens, such that they can still be used to evaluate frontier LCLMs.\nReliable evaluation\n: Many existing benchmarks still use n-gram-based metrics, such as ROUGE, despite their poor correlation with human judgments (\nGoyal et al., 2023\n). We employ model-based evaluations that show better distinguishability between models and different input lengths (Figure 3). Furthermore, our human studies show that our metrics have a high agreement with human judgments.\nFigure 3: ROUGE cannot differentiate between models and lengths, while model-based evaluations are better at separating models of different capacities.\nRobust prompting\n: Existing long-context benchmarks often require models to follow instructions, but many model developments revolve around base models, which have to rely on synthetic tasks or perplexity for evaluation. Thus, we support base models for a subset of our tasks via in-context learning examples. This substantially improves the performance of base models, which is more reflective of real-world applications.\nLCLMs still have a long way to go on real-world tasks\nOur experiments and analyses include a comprehensive set of 59 LCLMs. To our knowledge, this is the most thorough and controlled comparison of long-context models on diverse applications. These models cover both leading proprietary and open-source models, and we also consider models with different architectures (e.g., full-attention transformers, hybrid architectures) and positional extrapolation techniques. In this section, we will highlight a few key findings from our experiments.\nDiverse evaluation is needed for assessing long-context abilities\nLong-context benchmarks are often constructed with specific applications in mind, such as summarization or question answering, which limits the understanding of LCLMs in a broader context. We examine model performance over a wide range of real tasks and find that different categories do not always correlate with each other (Figure 4).\nFigure 4: Different categories do not correlate well with each other.\nWhile some tasks moderately correlate with each other (e.g., RAG and MS-MARCO) due to their retrieval-based nature, others show little correlation (e.g., Summ and Cite). Notably, ICL has the lowest correlation with other tasks, which suggests that it is a unique task that requires different capabilities from the model. Therefore, model developers should evaluate across these distinct axes to draw a more holistic picture of the model's capabilities.\nModels degrade with increasing lengths and task complexity\nWe present the results of the frontier proprietary models as well as a few open-source models on HELMET.\nAdditional results can be found in the paper and the website.\nFigure 5: HELMET results on selected instruction-tuned models across tasks and input lengths.\nFirst, we observe that\nopen-source models lag behind closed-source models on complex tasks\n. Although the gap appears small on simpler tasks, such as Recall, the gap widens on more complex ones, such as Cite.\nFurthermore,\nperformance degradation with increasing lengths is category-dependent\n. Even the most advanced models, such as GPT-4o and Gemini, experience a significant decrease in performance on tasks like re-ranking. This change in performance cannot be observed from simply looking at the synthetic task performance.\nFinally,\nthere is no clear winner across all categories\n, thereby calling for evaluation across different axes. Additional analysis, such as the performance of different positional extrapolation methods and the lost-in-the-middle phenomenon, can be found in the paper.\nUsing HELMET for future developments\nHow to run HELMET\nUsing HELMET is easy! Simply clone our\nGitHub repository\n, and everything is ready to go after setting up the environment!\nWe provide many different ways for loading models, which can be configured in the config file:\nusing HuggingFace's\ntransformers\nlibrary\nusing HuggingFace's TGI to launch a model endpoint in your machine\nusing HuggingFace's Inference Endpoints to launch a remote model endpoint\nusing vllm to launch a model endpoint in your machine. Note: You can launch vllm endpoint on Intel Gaudi accelerators.\nusing model provider's APIs\nOption 1. Using HuggingFace's\ntransformers\nlibrary\nJust use the config yamls in our repo and run these evaluations with\npython eval.py --config configs/rag.yaml --model_name_or_path <model_name>\nBehind the scenes, HuggingFace's\ntransformers\nlibrary is used, and both local and remote models are automatically supported.\nOption 2. Using HuggingFace's TGI\nFirst, follow the instructions on\nTGI github\nto launch a model endpoint. Then in your config file, specify the endpoint url. For example, you can have a config.yaml like below\ninput_max_length: 131072\ndatasets: kilt_nq\ngeneration_max_length: 20\ntest_files: data/kilt/nq-dev-multikilt_1000_k1000_dep6.jsonl\ndemo_files: data/kilt/nq-train-multikilt_1000_k3_dep6.jsonl\nuse_chat_template: true\nmax_test_samples: 100\nshots: 2\nstop_new_line: true\nmodel_name_or_path: tgi:meta-llama/Llama-3.1-8B-Instruct # need to add \"tgi:\" prefix\nuse_tgi_serving: true # add this line in your config\nThen use the command below to run the benchmark\nexport\nLLM_ENPOINT=<your-tgi-endpoint>\n# example: \"https://10.10.10.1:8080/v1\"\npython eval.py --config configs/config.yaml --endpoint_url\n$LLM_ENDPOINT\nOption 3. Using HuggingFace's Inference Endpoints\nFirst set up an endpoint by following the instructions\nhere\n. Get the endpoint url and your API key. Then use the same config yaml shown in Option 2 above, and run the command below.\nexport\nLLM_ENPOINT=<your-hf-inference-endpoint>\n# example: \"https://XXXX.us-east-1.aws.endpoints.huggingface.cloud/v1\"\nexport\nAPI_KEY=<your-hf-api-key>\npython eval.py --config configs/config.yaml --endpoint_url\n$LLM_ENDPOINT\n--api_key\n$API_KEY\nOption 4. Using VLLM\nYou can launch a model endpoint with vllm on your system, including Intel Gaudi2 and Gaudi3 accelerators. See the instructions\nhere\non how to run HELMET using vllm on Intel Gaudi accelerators.\nYou can use the same example config.yaml as in Option 2, except for two lines of change as below:\nmodel_name_or_path: meta-llama/Llama-3.1-8B-Instruct # no prefix needed\nuse_vllm_serving: true # use vllm instead of tgi\nThen use the command below to run the benchmark\nexport\nLLM_ENPOINT=<your-vllm-endpoint>\npython eval.py --config configs/config.yaml --endpoint_url\n$LLM_ENDPOINT\nOption 5. Using Model Provider's APIs\nWe support APIs from OpenAI, Anthropic, Google, and TogetherAI.\nPlease refer to the instructions in our\nrepo\n.\nFaster development\nWe recommend using the Recall and RAG tasks for fast iterations during model development.\nThese tasks achieve a good balance between fast evaluation and correlation with other realistic tasks.\nYou can easily run these evaluations with just\npython eval.py --config configs/rag.yaml --model_name_or_path <model_name>\nQuick comparison with existing models\nIt is often expensive to run all the baselines for evaluating LCLMs, especially at long contexts given their computational and memory costs.\nFor example, running HELMET at all lengths on a 70B model requires a node with 8 * 80GB GPUs for hundreds of GPU hours, which can be costly. \nBy evaluating on HELMET, researchers can directly compare their models to existing ones simply by referencing our results, which cover 59 models of different sizes and architectures.\nYou can find the leaderboard on our\nwebsite\n.\nLooking ahead\nHELMET is a step towards a more comprehensive evaluation of long-context language models, but there are still many more exciting applications of LCLMs. \nFor example, we recently released\nLongProc\n, a benchmark for evaluating LCLMs on\nlong-form generation\nand\nfollowing procedures\n, which are critical for developing reasoning models that generate tens of thousands of tokens in thinking steps.\nAlthough summarization tasks have long outputs (up to 1K tokens), LongProc focuses on even longer outputs, up to 8K tokens. \nSimilar to HELMET, LongProc is also designed with reliable evaluation settings and diverse tasks. \nWe are working on integrating LongProc into HELMET's evaluation suite, and we hope that this will provide a more comprehensive evaluation of LCLMs on long-form tasks.\nAcknowledgements\nWe thank Mengzhou Xia, Howard Chen, Xi Ye, Yinghui He, Lucy He, Alexander Wettig, Sadhika Malladi, Adithya Bhaskar, Joie Zhang, and other members of the Princeton Language and Intelligence (PLI) group for their helpful feedback.\nThis work is gratefully supported by the Microsoft Accelerate Foundation Models Research (AFMR) for Azure OpenAI credits and an Intel grant.\nCitation\nIf you find HELMET useful, please consider citing our paper:\n@inproceedings{yen2025helmet,\n      title={HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly}, \n      author={Howard Yen and Tianyu Gao and Minmin Hou and Ke Ding and Daniel Fleischer and Peter Izsak and Moshe Wasserblat and Danqi Chen},\n      year={2025},\n      booktitle={International Conference on Learning Representations (ICLR)},\n}\nMore Articles from our Blog\nWelcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM\nBy\nariG23498\nMarch 12, 2025\n•\n399\nFinally, a Replacement for BERT: Introducing ModernBERT\nBy\nbclavie\nDecember 19, 2024\nguest\n•\n611\nCommunity\nFranck-Dernoncourt\n4 days ago\nThanks for sharing! Another benchmark to evaluate long contexts:\nhttps://github.com/adobe-research/NoLiMa\n; paper:\nNoLiMa: Long-Context Evaluation Beyond Literal Matching\nSee translation\nReply\nEdit\nPreview\nUpload images, audio, and videos by dragging in the text input, pasting, or\nclicking here\n.\nTap or paste here to upload images\nYour need to confirm your account before you can post a new comment.\nComment\n·\nSign up\nor\nlog in\nto comment\nUpvote\n22\n+10"
}