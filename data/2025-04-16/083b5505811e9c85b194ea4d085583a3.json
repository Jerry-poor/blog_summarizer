{
  "title": "Cohere on Hugging Face Inference Providers üî•",
  "link": "https://huggingface.co/blog/inference-providers-cohere",
  "published": "2025-04-16",
  "summary": "",
  "content": "Back to Articles\nCohere on Hugging Face Inference Providers üî•\nPublished\n\t\t\t\tApril 16, 2025\nUpdate on GitHub\nUpvote\n119\n+113\nreach-vb\nVaibhav Srivastav\nburtenshaw\nben burtenshaw\nmerve\nMerve Noyan\ncelinah\nC√©lina Hanouti\nalexrs\nAlejandro Rodriguez\nCohereLabs\njulien-c\nJulien Chaumond\nsbrandeis\nSimon Brandeis\nCohere Models\nCohereLabs/c4ai-command-a-03-2025 üîó\nCohereLabs/aya-expanse-32b üîó\nCohereLabs/c4ai-command-r7b-12-2024 üîó\nCohereLabs/aya-vision-32b üîó\nHow it works\nIn the website UI\nFrom the client SDKs\nFrom OpenAI client\nTool Use with Cohere Models\nBilling\nWe're thrilled to share that\nCohere\nis now a supported Inference Provider on HF Hub! This also marks the first model creator to share and serve their models directly on the Hub.\nCohere\nis committed to building and serving models purpose-built for enterprise use-cases. Their comprehensive suite of secure AI solutions, from cutting-edge Generative AI to powerful Embeddings and Ranking models, are designed to tackle real-world business challenges. Additionally,\nCohere Labs\n, Cohere‚Äôs in house research lab, supports fundamental research and seeks to change the spaces where research happens.\nStarting now, you can run serverless inference to the following models via Cohere and Inference Providers:\nCohereLabs/c4ai-command-r-v01\nCohereLabs/c4ai-command-r-plus\nCohereLabs/c4ai-command-r-08-2024\nCohereLabs/c4ai-command-r7b-12-2024\nCohereLabs/c4ai-command-a-03-2025\nCohereLabs/aya-expanse-8b\nCohereLabs/aya-expanse-32b\nCohereLabs/aya-vision-8b\nCohereLabs/aya-vision-32b\nLight up your projects with Cohere and Cohere Labs today!\nCohere Models\nCohere and Cohere Labs bring a swathe of their models to Inference Providers that excel at specific business applications. Let‚Äôs explore some in detail.\nCohereLabs/c4ai-command-a-03-2025\nüîó\nOptimized for demanding enterprises that require fast, secure, and high-quality AI. Its 256k context length (2x most leading models) can handle much longer enterprise documents. Other key features include Cohere‚Äôs advanced retrieval-augmented generation (RAG) with verifiable citations, agentic tool use, enterprise-grade security, and strong multilingual performance (support for 23 languages).\nCohereLabs/aya-expanse-32b\nüîó\nFocuses on state-of-the-art multilingual support, applying the latest research on multilingual pre-training. Supports Arabic, Chinese (simplified & traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian, and Vietnamese with 128K context length.\nCohereLabs/c4ai-command-r7b-12-2024\nüîó\nIdeal for low-cost or low-latency use cases, bringing state-of-the-art performance in its class of open-weight models across real-world tasks. This model offers a context length of 128k. It delivers a powerful combination of multilingual support, citation-verified retrieval-augmented generation (RAG), reasoning, tool use, and agentic behavior. Also supports 23 languages.\nCohereLabs/aya-vision-32b\nüîó\n32-billion parameter model with advanced capabilities optimized for a variety of vision-language use cases, including OCR, captioning, visual reasoning, summarization, question answering, code, and more. It expands multimodal capabilities to 23 languages spoken by over half the world's population.\nHow it works\nYou can use Cohere models directly on the Hub either on the website UI or via the client SDKs.\nYou can find all the examples mentioned in this section on the\nCohere documentation page\n.\nIn the website UI\nYou can search for Cohere models by filtering by the inference provider in the\nmodel hub\n.\nFrom the Model Card, you can select the inference provider and run inference directly in the UI.\nFrom the client SDKs\nLet‚Äôs walk through using Cohere models from client SDKs. We‚Äôve also made a\ncolab notebook\nwith these snippets, in case you want to try them out right away.\nfrom Python, using huggingface_hub\nThe following example shows how to use Command A using Cohere as your inference provider. You can use a\nHugging Face token\nfor automatic routing through Hugging Face, or your own cohere API key if you have one.\nInstall\nhuggingface_hub\nv0.30.0 or later:\npip install -U\n\"huggingface_hub>=0.30.0\"\nUse the\nhuggingface_hub\npython library to call Cohere endpoints by defining the\nprovider\nparameter.\nfrom\nhuggingface_hub\nimport\nInferenceClient\n\nclient = InferenceClient(\n    provider=\n\"cohere\"\n,\n    api_key=\n\"xxxxxxxxxxxxxxxxxxxxxxxx\"\n,\n)\n\nmessages = [\n        {\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"How to make extremely spicy Mayonnaise?\"\n}\n]\n\ncompletion = client.chat.completions.create(\n    model=\n\"CohereLabs/c4ai-command-r7b-12-2024\"\n,\n    messages=messages,\n    temperature=\n0.7\n,\n    max_tokens=\n512\n,\n)\nprint\n(completion.choices[\n0\n].message)\nAya Vision, Cohere Labs‚Äô multilingual, multimodal model is also supported. You can include images encoded in base64 as follows:\nimage_path =\n\"img.jpg\"\nwith\nopen\n(image_path,\n\"rb\"\n)\nas\nf:\n    base64_image = base64.b64encode(f.read()).decode(\n\"utf-8\"\n)\nimage_url =\nf\"data:image/jpeg;base64,\n{base64_image}\n\"\nfrom\nhuggingface_hub\nimport\nInferenceClient\n\nclient = InferenceClient(\n    provider=\n\"cohere\"\n,\n    api_key=\n\"xxxxxxxxxxxxxxxxxxxxxxxx\"\n,\n)\n\nmessages = [\n        {\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: [\n                {\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"What's in this image?\"\n},\n                {\n\"type\"\n:\n\"image_url\"\n,\n\"image_url\"\n: {\n\"url\"\n: image_url},\n                },\n            ]\n        }\n]\n\ncompletion = client.chat.completions.create(\n    model=\n\"CohereLabs/aya-vision-32b\"\n,\n    messages=messages,\n    temperature=\n0.7\n,\n    max_tokens=\n512\n,\n)\nprint\n(completion.choices[\n0\n].message)\nfrom JS using @huggingface/inference\nimport\n{\nHfInference\n}\nfrom\n\"@huggingface/inference\"\n;\nconst\nclient =\nnew\nHfInference\n(\n\"xxxxxxxxxxxxxxxxxxxxxxxx\"\n);\nconst\nchatCompletion =\nawait\nclient.\nchatCompletion\n({\nmodel\n:\n\"CohereLabs/c4ai-command-a-03-2025\"\n,\nmessages\n: [\n        {\nrole\n:\n\"user\"\n,\ncontent\n:\n\"How to make extremely spicy Mayonnaise?\"\n}\n    ],\nprovider\n:\n\"cohere\"\n,\nmax_tokens\n:\n512\n});\nconsole\n.\nlog\n(chatCompletion.\nchoices\n[\n0\n].\nmessage\n);\nFrom OpenAI client\nHere's how you can call Command R7B using Cohere as the inference provider via the OpenAI client library.\nfrom\nopenai\nimport\nOpenAI\n\nclient = OpenAI(\n    base_url=\n\"https://router.huggingface.co/cohere/compatibility/v1\"\n,\n    api_key=\n\"xxxxxxxxxxxxxxxxxxxxxxxx\"\n,\n)\n\nmessages = [\n        {\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"How to make extremely spicy Mayonnaise?\"\n}\n]\n\ncompletion = client.chat.completions.create(\n    model=\n\"command-a-03-2025\"\n,\n    messages=messages,\n    temperature=\n0.7\n,\n)\nprint\n(completion.choices[\n0\n].message)\nTool Use with Cohere Models\nCohere‚Äôs models bring state-of-the-art agentic tool use to Inference Providers so let‚Äôs explore that in detail. Both the Hugging Face Hub client and the OpenAI client are compatible with tools via inference providers, so the above examples can be expanded.\nFirst, we will need to define tools for the model to use. Below we define the\nget_flight_info\nwhich calls an API for the latest flight information using two locations. This tool definition will be represented by the model‚Äôs chat template. Which we can also explore in the\nmodel card\n(üéâ open source).\ntools = [\n    {\n\"type\"\n:\n\"function\"\n,\n\"function\"\n: {\n\"name\"\n:\n\"get_flight_info\"\n,\n\"description\"\n:\n\"Get flight information between two cities or airports\"\n,\n\"parameters\"\n: {\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"loc_origin\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The departure airport, e.g. MIA\"\n,\n                    },\n\"loc_destination\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The destination airport, e.g. NYC\"\n,\n                    },\n                },\n\"required\"\n: [\n\"loc_origin\"\n,\n\"loc_destination\"\n],\n            },\n        },\n    }\n]\nNext, we‚Äôll need to pass messages to the inference client for the model to use the tools when relevant. In the example below we define the assistant‚Äôs tool call in\ntool_calls,\nfor the sake of clarity.\nmessages = [\n    {\n\"role\"\n:\n\"developer\"\n,\n\"content\"\n:\n\"Today is April 30th\"\n},\n    {\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"When is the next flight from Miami to Seattle?\"\n,\n    },\n    {\n\"role\"\n:\n\"assistant\"\n,\n\"tool_calls\"\n: [\n            {\n\"function\"\n: {\n\"arguments\"\n:\n'{ \"loc_destination\": \"Seattle\", \"loc_origin\": \"Miami\" }'\n,\n\"name\"\n:\n\"get_flight_info\"\n,\n                },\n\"id\"\n:\n\"get_flight_info0\"\n,\n\"type\"\n:\n\"function\"\n,\n            }\n        ],\n    },\n    {\n\"role\"\n:\n\"tool\"\n,\n\"name\"\n:\n\"get_flight_info\"\n,\n\"tool_call_id\"\n:\n\"get_flight_info0\"\n,\n\"content\"\n:\n\"Miami to Seattle, May 1st, 10 AM.\"\n,\n    },\n]\nFinally, the tools and messages are passed to the create method.\nfrom\nhuggingface_hub\nimport\nInferenceClient\n\nclient = InferenceClient(\n    provider=\n\"cohere\"\n,\n    api_key=\n\"xxxxxxxxxxxxxxxxxxxxxxxx\"\n,\n)\n\ncompletion = client.chat.completions.create(\n    model=\n\"CohereLabs/c4ai-command-r7b-12-2024\"\n,\n    messages=messages,\n    tools=tools,\n    temperature=\n0.7\n,\n    max_tokens=\n512\n,\n)\nprint\n(completion.choices[\n0\n].message)\nBilling\nFor direct requests, i.e. when you use a Cohere key, you are billed directly on your Cohere account.\nFor routed requests, i.e. when you authenticate via the Hub, you'll only pay the standard Cohere API rates. There's no additional markup from us, we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\nImportant Note ‚ÄºÔ∏è PRO users get $2 worth of Inference credits every month. You can use them across providers. üî•\nSubscribe to the\nHugging Face PRO plan\nto get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\nMore Articles from our Blog\nIntroducing HUGS - Scale your AI with Open Models\nBy\nphilschmid\nOctober 23, 2024\n‚Ä¢\n37\nBuild AI on premise with Dell Enterprise Hub\nBy\njeffboudier\nMay 21, 2024\n‚Ä¢\n24\nCommunity\nborgr\n9 days ago\n‚Ä¢\nedited 9 days ago\nGreat announcement!\nIs there a way to opt in to share my data with the world when using the UI? Or get all my conversations with an API request (so others\\we can build this opt in, in some hacky way)?\nSee translation\nReply\nmerve\nArticle author\n8 days ago\n‚Ä¢\nedited 8 days ago\n@\nborgr\nhello! as of now there's no such option, but we'll consider this, you want this for data labelling right? ‚ò∫Ô∏è for now you can use the providers programmatically and store them yourself I think\nSee translation\n1 reply\n¬∑\nborgr\n7 days ago\nLabelling, studying what people lack, learning about human reactions to various LM behavior etc.\nSee translation\nMohamedGhanySaleh\n7 days ago\nUuu\nReply\nDESSEP\n5 days ago\nThis comment has been hidden (marked as Off-Topic)\nDolzikov\n3 days ago\nThis comment has been hidden (marked as Off-Topic)\nMoMaged\nabout 20 hours ago\n‚Ä¢\nedited about 20 hours ago\n.\nReply\nEdit\nPreview\nUpload images, audio, and videos by dragging in the text input, pasting, or\nclicking here\n.\nTap or paste here to upload images\nYour need to confirm your account before you can post a new comment.\nComment\n¬∑\nSign up\nor\nlog in\nto comment\nUpvote\n119\n+107"
}