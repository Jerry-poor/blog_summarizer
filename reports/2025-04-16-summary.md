# HF 博客日报 — 2025-04-16

以下共有 3 篇博文，摘要如下：

## [17 Reasons Why Gradio Isn't Just Another UI Library](https://huggingface.co/blog/why-gradio-stands-out) （2025-04-16）

**Abstract:**

Gradio is more than a UI library; it's a framework for interacting with machine learning models.
*Gradio is more than a UI library; it's a framework for interacting with machine learning models.*

It offers Universal API Access, including Python (gradio_client) and JavaScript (@gradio/client) SDKs, and automatic REST API generation.
*It offers Universal API Access, including Python (gradio_client) and JavaScript (@gradio/client) SDKs, and automatic REST API generation.*

Features include interactive API recording, server-side rendering, automatic queue management, high-performance streaming, multi-page support, and client-side Groovy execution.
*Features include interactive API recording, server-side rendering, automatic queue management, high-performance streaming, multi-page support, and client-side Groovy execution.*

Gradio provides a comprehensive theming system, dynamic interfaces, visual development with Sketch, PWA support, in-browser execution with Lite, AI-assisted tooling, hassle-free sharing, enterprise-grade security, enhanced Dataframe component, and Deep Links.
*Gradio provides a comprehensive theming system, dynamic interfaces, visual development with Sketch, PWA support, in-browser execution with Lite, AI-assisted tooling, hassle-free sharing, enterprise-grade security, enhanced Dataframe component, and Deep Links.*

These features enable building powerful and secure AI applications with performance guarantees.
*These features enable building powerful and secure AI applications with performance guarantees.*

## Abstract

**摘要**

在JS Web框架中实现SSR渲染需要广泛的全栈开发知识。
*Rendering while implementing SSR in JS web frameworks requires extensive full-stack development expertise.*

Gradio在保持纯Python开发体验的同时，提供了Web框架级别的性能（注意：除了需要安装Node！）。
*Gradio delivers web framework-level performance while maintaining a pure Python development experience (Note: except for having to install Node!).*

Gradio为ML应用提供了一个复杂的队列系统，处理GPU密集型计算和高流量用户访问。
*Gradio provides a sophisticated queuing system tailored for ML applications that handles both GPU-intensive computations and high-volume user access.*

Gradio的队列自动处理应用中定义的不同类型的任务，无论是GPU上运行的长预测，音频/视频流，还是非ML任务。
*Gradio's queue automatically handles different kinds of tasks defined in your application, whether they are long predictions that run on a GPU, audio/video streaming, or non-ML tasks.*

您的应用可以扩展到数千并发用户，而不会出现资源争用和系统过载。
*Your applications can scale to thousands of concurrent users without resource contention and system overwhelming.*

通过服务器发送事件 (Server-Side Events) 提供实时队列状态更新，向用户展示他们在队列中的当前位置。
*Real-time queue status updates via Server-Side Events, showing users their current position in the queue.*

Gradio的流式传输能力通过Python生成器和`yield`语句实现，通过 HTTP Live Streaming (HLS) 协议支持实时、低延迟的更新，用于token-by-token文本生成、图像生成更新，或音频/视频流。
*Gradio's streaming capabilities, implemented via Python generators and `yield` statements, support real-time, low-latency updates via HTTP Live Streaming (HLS) protocol for token-by-token text generation, image generation updates, or audio/video streaming.*

```
**Abstract:**

Gradio 5 significantly enhances real-time streaming capabilities with FastRTC, enabling Python-based audio/video streaming applications.
Gradio 5 significantly enhances real-time streaming capabilities with FastRTC, enabling Python-based audio/video streaming applications.

Unlike other frameworks requiring manual thread management, Gradio offers seamless streaming.
Unlike other frameworks requiring manual thread management, Gradio offers seamless streaming.

Native multi-page support allows for building comprehensive AI/ML applications, featuring automatic URL routing.
Native multi-page support allows for building comprehensive AI/ML applications, featuring automatic URL routing.

Backend resources are shared across pages, improving file maintainability.
Backend resources are shared across pages, improving file maintainability.

Gradio simplifies routing compared to other frameworks requiring explicit setup.
Gradio simplifies routing compared to other frameworks requiring explicit setup.

Groovy introduces Python-to-JavaScript transpilation for instant UI responsiveness via `js=True`.
Groovy introduces Python-to-JavaScript transpilation for instant UI responsiveness via `js=True`.

This reduces server load and latency, particularly beneficial for high-traffic applications.
This reduces server load and latency, particularly beneficial for high-traffic applications.
```

**Abstract**

Gradio provides a single-language Python development experience, automatically transpiling to JavaScript for web-native performance, unlike frameworks requiring separate JavaScript codebases.
Gradio 提供单一语言的 Python 开发体验，自动转译为 JavaScript 以实现 Web 原生性能，不同于需要单独 JavaScript 代码库的框架。

Its comprehensive theming system and ready-to-use theme presets (Monochrome, Soft, Ocean, Glass) offer professional UI, including built-in dark mode and mobile responsiveness.
其全面的主题系统和即用型主题预设（Monochrome、Soft、Ocean、Glass）提供专业的 UI，包括内置的暗黑模式和移动响应能力。

ML-specific UI components (ImageEditor, ImageSlider) and enhanced features for LLMs and Agents are included.
包含 ML 特定的 UI 组件（ImageEditor，ImageSlider）以及 LLM 和 Agent 的增强功能。

Gradio's `@gr.render()` facilitates dynamic interfaces.
Gradio 的 `@gr.render()` 有助于动态界面。

It enables ML practitioners to create polished applications without web design expertise.
它使 ML 从业者无需 Web 设计专业知识即可创建精美的应用程序。

Other frameworks offer limited color customization requiring manual CSS implementation.
其他框架提供的颜色自定义功能有限，需要手动实现 CSS。

**Abstract:**

Gradio now supports dynamic UI modifications based on user interaction and state.
*Gradio now supports dynamic UI modifications based on user interaction and state.*

Developers can render UI on-the-fly based on model outputs, utilizing the `.render()` method for rendering Blocks within Blocks.
*Developers can render UI on-the-fly based on model outputs, utilizing the `.render()` method for rendering Blocks within Blocks.*

Gradio Sketch introduces a WYSIWYG editor for no-code ML application design, enabling visual layout creation, event definition, and automatic code generation.
*Gradio Sketch introduces a WYSIWYG editor for no-code ML application design, enabling visual layout creation, event definition, and automatic code generation.*

Compared to other frameworks requiring JavaScript or manual coding for UI updates, Gradio simplifies dynamic UI manipulation with Python and democratizes AI development.
*Compared to other frameworks requiring JavaScript or manual coding for UI updates, Gradio simplifies dynamic UI manipulation with Python and democratizes AI development.*

Gradio also offers Progressive Web App (PWA) support.
*Gradio also offers Progressive Web App (PWA) support.*

```
**Abstract:**

Gradio facilitates the creation of Progressive Web Apps (PWAs) for mobile and desktop, eliminating the need for extra configurations.
Gradio 可以轻松创建适用于移动和桌面平台的渐进式 Web 应用 (PWAs)，无需额外配置。
This capability expands user access, enabling instant mobile app creation.
此功能扩展了用户访问范围，能够即时创建移动应用程序。
Gradio Lite supports in-browser execution via Pyodide (WebAssembly), enabling client-side model inference using Transformers.js and ONNX, enhancing privacy and eliminating server costs.
Gradio Lite 支持通过 Pyodide (WebAssembly) 在浏览器端执行，可以使用 Transformers.js 和 ONNX 进行客户端模型推理，从而增强隐私并消除服务器成本。
Unlike other frameworks, Gradio allows serverless deployment of Python ML applications, even on static hosting services.
与其他框架不同，Gradio 允许 Python 机器学习应用程序的无服务器部署，即使在静态托管服务上也是如此。
Gradio accelerates development with features like hot reload and AI Playground for natural language-driven app generation, supporting rapid prototyping with `HuggingFace` integrations.
Gradio 通过热重载和用于自然语言驱动应用程序生成的 AI Playground 等功能加速开发，支持使用 `HuggingFace` 集成进行快速原型设计。
```

Absolutely! Here's the abstract, adhering to your specifications:

**Abstract**

Gradio facilitates rapid ML application development using `gr.load()` and OpenAI-compatible API endpoints.
_Gradio facilitates rapid ML application development using `gr.load()` and OpenAI-compatible API endpoints._

It offers instant UI feedback and AI-assisted tooling, enabling researchers and developers to quickly iterate.
_It offers instant UI feedback and AI-assisted tooling, enabling researchers and developers to quickly iterate._

Unlike other frameworks requiring manual refreshes or complex build pipelines, Gradio provides immediate code update reflection.
_Unlike other frameworks requiring manual refreshes or complex build pipelines, Gradio provides immediate code update reflection._

Hassle-free app sharing is achieved via `demo.launch(share=True)`, generating a public URL on the `*.gradio.live` domain for one week, tunneled securely via Fast Reverse Proxy (FRP).
_Hassle-free app sharing is achieved via `demo.launch(share=True)`, generating a public URL on the `*.gradio.live` domain for one week, tunneled securely via Fast Reverse Proxy (FRP)._

For extended availability and custom domains, self-hosting an FRP server is an option, bypassing the default 168-hour timeout.
_For extended availability and custom domains, self-hosting an FRP server is an option, bypassing the default 168-hour timeout._

**Word Count:** 120 words

---

## [Cohere on Hugging Face Inference Providers 🔥](https://huggingface.co/blog/inference-providers-cohere) （2025-04-16）

**Abstract:**

We announce Cohere's integration as a supported Inference Provider on Hugging Face Hub.
我们宣布 Cohere 已集成到 Hugging Face Hub 作为支持的推理提供商。

This marks the first model creator to directly share and serve models on the Hub.
这标志着第一个模型创建者直接在 Hub 上共享和提供模型。

Cohere offers secure AI solutions for enterprise use, including Generative AI, Embeddings, and Ranking models.
Cohere 为企业用途提供安全的 AI 解决方案，包括生成式 AI、嵌入和排序模型。

CohereLabs supports fundamental research and innovation.
CohereLabs 支持基础研究和创新。

Users can now run serverless inference on models like *CohereLabs/c4ai-command-a-03-2025* and *CohereLabs/aya-expanse-32b*.
用户现在可以在*CohereLabs/c4ai-command-a-03-2025*和*CohereLabs/aya-expanse-32b*等模型上运行无服务器推理。

*CohereLabs/c4ai-command-a-03-2025* is optimized for enterprises needing fast, secure, high-quality AI with a 256k context length.
*CohereLabs/c4ai-command-a-03-2025* 针对需要快速、安全、高质量 AI 且上下文长度为 256k 的企业进行了优化。

Leverage Cohere and Cohere Labs for advanced AI applications.
利用 Cohere 和 Cohere Labs 实现高级 AI 应用程序。

**摘要 (Abstract)**

本文介绍Cohere的先进检索增强生成 (RAG) 模型，具有可验证的引文、智能体工具使用、企业级安全性和强大的多语言性能（支持 23 种语言）。
This paper introduces Cohere's advanced retrieval-augmented generation (RAG) models with verifiable citations, agentic tool use, enterprise-grade security, and strong multilingual performance (support for 23 languages).

CohereLabs/aya-expanse-32b 专注于最先进的多语言支持，应用了最新的多语言预训练研究。
CohereLabs/aya-expanse-32b focuses on state-of-the-art multilingual support, applying the latest research on multilingual pre-training.

CohereLabs/c4ai-command-r7b-12-2024 适用于低成本或低延迟用例，在同类开源模型中提供最先进的性能，并具有 128K 的上下文长度。
CohereLabs/c4ai-command-r7b-12-2024 is ideal for low-cost or low-latency use cases, bringing state-of-the-art performance in its class of open-weight models, with a context length of 128K.

CohereLabs/aya-vision-32b 是一个 320 亿参数模型，针对各种视觉-语言用例进行了优化，扩展了多模态功能至 23 种语言。
CohereLabs/aya-vision-32b is a 32-billion parameter model optimized for vision-language use cases, expanding multimodal capabilities to 23 languages.

可以通过 Hugging Face Hub 的网页 UI 或客户端 SDK 直接使用 Cohere 模型。
Cohere models can be directly used on the Hub either on the website UI or via the client SDKs.

**Abstract:**

本文介绍了如何使用 Hugging Face Hub 作为接口，调用 Cohere 的推理服务，包括文本生成和多模态处理。
This paper introduces how to utilize Hugging Face Hub as an interface to invoke Cohere's inference services, including text generation and multimodal processing.

通过设置 `provider="cohere"` 和 API 密钥，可以轻松地调用 Cohere 的端点。
By setting `provider="cohere"` and the API key, Cohere's endpoints can be easily called.

文本生成示例展示了如何使用 `InferenceClient` 和 `chat.completions.create()` 方法，使用如 "CohereLabs/c4ai-command-r7b-12-2024" 等模型进行对话。
The text generation example demonstrates how to use the `InferenceClient` and `chat.completions.create()` methods, using models like "CohereLabs/c4ai-command-r7b-12-2024" for dialogue.

多模态处理示例则展示了如何通过 base64 编码嵌入图像，并使用 "CohereLabs/aya-vision-32b" 等模型进行图像理解，利用 `image_url = f"data:image/jpeg;base64,{base64_image}"` 传递图像数据。
The multimodal processing example demonstrates how to embed images via base64 encoding and utilize models such as "CohereLabs/aya-vision-32b" for image understanding, passing image data using `image_url = f"data:image/jpeg;base64,{base64_image}"`.

**Abstract**

This work demonstrates calling Cohere's Command R7B model using the OpenAI client library via inference providers.
本研究展示了如何通过Inference Providers使用OpenAI客户端库调用Cohere的Command R7B模型。

The implementation utilizes a custom base URL (e.g., `"https://router.huggingface.co/cohere/compatibility/v1"`) and API key for authentication.
该实现使用自定义的 base URL (例如, `"https://router.huggingface.co/cohere/compatibility/v1"`) 和 API 密钥进行身份验证。

Furthermore, we explore agentic tool use, defining a `get_flight_info` function with parameters for origin (`loc_origin`) and destination (`loc_destination`) airports.
此外，我们探索了agentic工具的使用，定义了一个 `get_flight_info` 函数，其参数包括起始机场(`loc_origin`)和目标机场(`loc_destination`)。

The tool definition is then incorporated into the model's chat template, enabling the model to make appropriate `tool_calls` during inference.
然后，工具定义被整合到模型的聊天模板中，使模型能够在推理过程中进行适当的 `tool_calls`。

**Abstract:**

This work demonstrates the integration of a large language model (LLM) for flight information retrieval using the Hugging Face InferenceClient. This work utilizes a Cohere model `CohereLabs/c4ai-command-r7b-12-2024`.
本文展示了使用Hugging Face InferenceClient集成大型语言模型(LLM)进行航班信息检索。This work utilizes a Cohere model `CohereLabs/c4ai-command-r7b-12-2024`.

The system processes a multi-turn conversation, including developer's date info, user's flight query (Miami to Seattle), and assistant's `tool_calls` using a function `get_flight_info` with arguments like `{ "loc_destination": "Seattle", "loc_origin": "Miami" }`.
该系统处理多轮对话，包括开发人员的日期信息、用户的航班查询（迈阿密到西雅图）以及助手使用函数`get_flight_info`的`tool_calls`，参数如`{ "loc_destination": "Seattle", "loc_origin": "Miami" }`。

The tool provides the flight information, `Miami to Seattle, May 1st, 10 AM.`, which returns to the LLM.
该工具提供了航班信息，`Miami to Seattle, May 1st, 10 AM.`，返回给LLM。

Billing information is provided, emphasizing the standard Cohere API rates and potential Inference credits for PRO users.
提供了账单信息，强调了标准的Cohere API费率和PRO用户的潜在Inference credits。

The paper also mentions related articles and community feedback.
本文还提及了相关文章和社区反馈。

**Abstract**

本文讨论了用户对聊天记录API访问的需求，以便进行数据标注和用户行为研究。
This paper discusses the user demand for API access to chat logs for data labeling and user behavior research.

目前，该功能尚未实现，但已被开发者考虑。
Currently, this functionality is not implemented but is under consideration by the developers.

用户建议通过API请求获取对话数据，以便构建自定义的数据处理方案。
Users suggest obtaining conversation data via API requests to build custom data processing solutions.

一种替代方案是用户可以通过编程方式使用providers，并将数据自行存储。
An alternative is for users to programmatically use providers and store the data themselves.

研究目的包括标注、分析用户需求以及研究人类对各种语言模型行为的反应，如学习人类反应 $R(LM)$。
Research purposes include labeling, analyzing user needs, and studying human reactions to various language model behavior, such as learning human reaction $R(LM)$.

---

## [Introducing HELMET](https://huggingface.co/blog/helmet) （2025-04-16）

**Abstract:**

评估长文本语言模型（LCLMs）既具有挑战性又至关重要。Evaluating long-context language models (LCLMs) is challenging but important. 现有评估过度依赖合成任务。Existing evaluations overly rely on synthetic tasks. 我们引入HELMET，旨在为LCLMs构建多样、可控和可靠的评估。We introduce HELMET to craft diverse, controllable, and reliable evaluations for LCLMs. 相较于现有基准，HELMET进行了关键改进。HELMET includes key improvements over existing benchmarks. 结果表明，LCLMs在真实世界任务中仍有很长的路要走，模型性能随长度和任务复杂性增加而下降。Results show that LCLMs still have a long way to go on real-world tasks, and models degrade with increasing lengths and task complexity. HELMET促进对LCLMs能力的更全面评估，并推动未来发展。HELMET facilitates a more comprehensive assessment of LCLM capabilities and promotes future developments. 可通过 `https://github.com/princeton-nlp/HELMET` 访问代码和数据。 Code & Data is available at `https://github.com/princeton-nlp/HELMET`.

**Abstract**

大型语言模型(LLMs)极大地扩展了其上下文窗口，但现有评测体系存在局限性。
Large language models (LLMs) have significantly expanded their context window, but existing evaluation methods have limitations.

现有基准测试显示出反直觉的趋势，例如较小的模型优于较大的模型 (例如，Llama-3 8B > 70B)。
Existing benchmarks show counterintuitive trends, such as smaller models outperforming larger ones (e.g., Llama-3 8B > 70B).

现有的长上下文语言模型 (LCLMs) 的评测过度依赖于合成任务，且模型开发者经常使用不同的数据集进行评测。
Existing evaluations of long-context language models (LCLMs) overly rely on synthetic tasks, and model developers often evaluate on different sets of datasets.

我们提出了HELMET (How to Evaluate Long-Context Models Effectively and Thoroughly)，一个全面的LCLMs评测基准，以解决现有基准的多样性、可控性和可靠性问题。
We propose HELMET (How to Evaluate Long-Context Models Effectively and Thoroughly), a comprehensive benchmark for evaluating LCLMs that addresses the diversity, controllability, and reliability issues of existing benchmarks.

通过评估59个最新的LCLMs，我们发现，跨多样化应用评估模型的能力至关重要，并且前沿的LCLMs在复杂任务上仍然存在局限性。
By evaluating 59 recent LCLMs, we find that it is crucial to evaluate models across diverse applications to understand their capabilities, and frontier LCLMs are still limited on complex tasks.

**Abstract:**

长文本语言模型(LCLMs)的常用评估方法是困惑度或如大海捞针(NIAH)等合成任务。
A common practice for evaluating long-context language models (LCLMs) is to use perplexity or synthetic tasks, such as needle-in-a-haystack (NIAH).

然而，近期研究表明，困惑度与下游性能相关性不佳(Fang et al., 2024)。
However, recent works have shown that perplexity does not correlate well with downstream performance (Fang et al., 2024).

NIAH等简单合成任务与实际任务相关性差，而更复杂的合成任务相关性更高。
Simple synthetic tasks, such as NIAH, do not correlate well with real-world performance, but the more complex synthetic tasks achieve higher correlation with real-world tasks.

现有实际应用基准测试如ZeroScrolls, LongBench, 和InfiniteBench仍存在下游任务覆盖不足、测试长度受限和指标不可靠等问题。
Existing benchmarks with realistic applications such as ZeroScrolls, LongBench, and InfiniteBench still face limitations: insufficient task coverage, inadequate length, and unreliable metrics.

因此，我们提出了HELMET以解决这些问题，提供全面的LCLM评估。
Thus, we propose HELMET to address these limitations and provide a comprehensive evaluation of LCLMs.

HELMET的设计目标是提供多样化的下游任务覆盖，可控的长度和复杂度，以及可靠的评估，支持基础模型和指令微调模型。
HELMET is designed with diverse downstream task coverage, controllable length/complexity, and reliable evaluation for both base and instruction-tuned models.

我们实验评估了8K到128K tokens的输入长度，HELMET可扩展到更长文本。
In our experiments, we evaluate on input length from 8K to 128K tokens, and HELMET is easily extended.

## HELMET基准测试集：学术摘要

HELMET includes diverse tasks such as retrieval-augmented generation, citation, and summarization.
HELMET包含多种任务，例如检索增强生成、引文生成和摘要生成。

We select datasets with naturally long contexts for real-world applications.
我们选择具有自然长上下文的数据集，以适应真实世界的应用。

These datasets are complemented with model-based evaluations and human studies for reliable evaluation.
这些数据集辅以基于模型的评估和人工评估，以确保评估的可靠性。

Input length is controlled by manipulating retrieved passages (RAG, Cite, Re-rank), demonstrations (ICL), or document length (LongQA, Summ).
输入长度可以通过操纵检索到的段落（RAG、Cite、Re-rank）、演示（ICL）或文档长度（LongQA、Summ）来控制。

LongQA and Summ use datasets with >100K token natural documents.
LongQA和Summ使用包含超过10万token的自然文档的数据集。

We employ model-based evaluations, showing better distinguishability compared to n-gram metrics (ROUGE). *Figure 3*.
我们采用基于模型的评估，相比于基于n-gram的指标（ROUGE），显示出更好的区分度。*图3*。

Human studies show high agreement with our evaluation metrics.
人工评估显示出与我们的评估指标高度一致。

We support base models with in-context learning, improving their performance on our tasks.
我们通过上下文学习支持基准模型，提高它们在任务上的表现。

**摘要（Abstract）**

长文本语言模型（LCLMs）在真实世界应用中仍有很长的路要走。
*LCLMs still have a long way to go on real-world applications.*

我们的实验和分析包括59个LCLMs的全面集合。
*Our experiments and analyses include a comprehensive set of 59 LCLMs.*

据我们所知，这是对不同应用上长文本模型最彻底和受控的比较。
*To our knowledge, this is the most thorough and controlled comparison of long-context models on diverse applications.*

评估长文本能力需要多样化的评估方式。
*Diverse evaluation is needed for assessing long-context abilities.*

长文本基准通常是针对特定应用构建的，限制了LCLMs在更广泛上下文中的理解。
*Long-context benchmarks are often constructed with specific applications in mind, which limits the understanding of LCLMs in a broader context.*

我们发现不同类别之间的表现并不总是相互关联（如图4）。
*We examine model performance over a wide range of real tasks and find that different categories do not always correlate with each other (Figure 4).*

模型性能随着长度的增加和任务复杂性的提高而下降。
*Models degrade with increasing lengths and task complexity.*

我们展示了前沿专有模型以及一些开源模型在HELMET上的结果（如图5）。
*We present the results of the frontier proprietary models as well as a few open-source models on HELMET (Figure 5).*

我们观察到开源模型在复杂任务上落后于闭源模型。
*First, we observe that open-source models lag behind closed-source models on complex tasks.*

**Abstract:**

我们介绍HELMET，一个用于评估长上下文模型性能的综合框架。
We introduce HELMET, a comprehensive framework for evaluating the performance of long-context models.

HELMET揭示了长上下文长度下性能退化的问题，例如在重排序任务中。
HELMET reveals performance degradation with increasing context lengths, such as in re-ranking tasks.

性能退化程度取决于任务类型。
Performance degradation is category-dependent.

即使是GPT-4o和Gemini等最先进的模型也存在显著的性能下降。
Even state-of-the-art models like GPT-4o and Gemini experience a significant performance decrease.

不同任务类别没有明确的“最佳”模型，需要跨不同维度进行评估。
There is no clear winner across all categories, thereby calling for evaluation across different axes.

HELMET易于使用，支持多种模型加载方式，包括`transformers`、TGI、Inference Endpoints、`vllm` 以及模型提供商的API。
HELMET is easy to use and supports various model loading methods, including `transformers`, TGI, Inference Endpoints, `vllm`, and model provider APIs.

通过简单的命令行 `python eval.py --config configs/rag.yaml --model_name_or_path <model_name>` 即可运行评估。
Evaluations can be run with a simple command: `python eval.py --config configs/rag.yaml --model_name_or_path <model_name>`.

**Abstract:**

本文档概述了使用多种推理服务进行评估的方法。
This document outlines methods for evaluation using various inference services.

包括通过 Text Generation Inference (TGI) (需要 `"tgi:"` 前缀 and `use_tgi_serving: true`)、HuggingFace Inference Endpoints、VLLM (`use_vllm_serving: true`)以及 OpenAI、Anthropic、Google 和 TogetherAI 等模型提供商的 API 进行模型评估。
Model evaluation is possible via Text Generation Inference (TGI) (requiring `"tgi:"` prefix and `use_tgi_serving: true`), HuggingFace Inference Endpoints, VLLM (`use_vllm_serving: true`), and Model Provider APIs (OpenAI, Anthropic, Google, TogetherAI).

通过设置 `LLM_ENPOINT` 和 API 密钥，并运行 `python eval.py --config configs/config.yaml --endpoint_url $LLM_ENDPOINT [--api_key $API_KEY]` 指令来启动评测。
Evaluation is initiated by setting `LLM_ENPOINT` and API keys, then running `python eval.py --config configs/config.yaml --endpoint_url $LLM_ENDPOINT [--api_key $API_KEY]`.

推荐使用 Recall 和 RAG 任务加速模型开发 (`python eval.py --config configs/rag.yaml --model_name_or_path <model_name>`).
Recall and RAG tasks are recommended for faster model development (`python eval.py --config configs/rag.yaml --model_name_or_path <model_name>`).

**Abstract**

长文本语言模型（LCLMs）的评估，尤其是在长上下文情况下，鉴于其计算和内存成本，极具挑战性。
Evaluating Long-Context Language Models (LCLMs), especially at long contexts, is challenging given their computational and memory costs.

例如，在70B模型上运行所有长度的HELMET需要一个具有8 * 80GB GPU的节点，耗费数百GPU小时，成本高昂。
For example, running HELMET at all lengths on a 70B model requires a node with 8 * 80GB GPUs for hundreds of GPU hours, which can be costly.

通过在HELMET上进行评估，研究人员只需参考我们的结果，即可直接将他们的模型与现有的59个不同大小和架构的模型进行比较。
By evaluating on HELMET, researchers can directly compare their models to existing ones simply by referencing our results, which cover 59 models of different sizes and architectures.

我们最近发布了LongProc，一个用于评估LCLMs在长文本生成（long-form generation）和遵循程序（following procedures）方面的基准。
We recently released LongProc, a benchmark for evaluating LCLMs on long-form generation and following procedures.

LongProc侧重于更长的输出，高达8K tokens，而不仅仅是摘要任务的1K tokens输出。
LongProc focuses on even longer outputs, up to 8K tokens, unlike summarization tasks with up to 1K tokens outputs.

我们正在努力将LongProc整合到HELMET的评估套件中，希望这将为LCLMs在长文本任务上提供更全面的评估。
We are working on integrating LongProc into HELMET's evaluation suite, and we hope that this will provide a more comprehensive evaluation of LCLMs on long-form tasks.

```
**中英对照摘要（Abstract - Bilingual Abstract）**

本文概述了近期关于大型语言模型(LLM) 的讨论，重点关注Google Gemma 3和ModernBERT。
This abstract outlines recent discussions on Large Language Models (LLMs), highlighting Google's Gemma 3 and ModernBERT.

Gemma 3被描述为多模态、多语言、长上下文的开源LLM。
Gemma 3 is described as a multimodal, multilingual, long-context open-source LLM.

ModernBERT 则被视为BERT的替代方案。
ModernBERT is presented as a replacement for BERT.

评论区提到了一个评估长上下文的新基准：NoLiMa，相关论文链接为https://github.com/adobe-research/NoLiMa。
A comment mentions a new benchmark for evaluating long contexts: NoLiMa, with the related paper available at https://github.com/adobe-research/NoLiMa.

该基准超越了字面匹配进行长上下文评估。
This benchmark goes beyond literal matching for long-context evaluation.

总体而言，讨论围绕着LLM的最新进展和评估方法展开。
Overall, the discussion revolves around the latest advancements and evaluation methods in LLMs.
```

---

