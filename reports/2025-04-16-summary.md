# HF 博客日报 — 2025-04-16

以下共有 3 篇博文，摘要如下：

## [17 Reasons Why Gradio Isn't Just Another UI Library](https://huggingface.co/blog/why-gradio-stands-out) （2025-04-16）

好的，下面是该文章的中英双语摘要，保留了公式与术语：

**摘要：**

**英文:**

This article highlights 17 reasons why Gradio is more than just a UI library, positioning it as a framework for interacting with machine learning models through UIs and APIs.  Gradio offers features such as universal API access through official SDKs in Python (`gradio_client`) and JavaScript (`@gradio/client`), as well as support for cURL API access, with automatic REST API endpoint generation. Other key features include interactive API recorders, server-side rendering for fast ML apps, automatic queue management, high-performance streaming for real-time outputs, integrated multi-page application support, client-side function execution with Groovy, comprehensive theming, dynamic interfaces, visual interface development with Gradio Sketch, PWA support, in-browser execution with Gradio Lite, AI-assisted tooling, hassle-free app sharing, enterprise-grade security, enhanced Dataframe component, and Deep Links for sharing app states. The article emphasizes Gradio's focus on performance, security, and responsiveness for building powerful AI applications.

**中文:**

本文强调了 Gradio 不仅仅是一个 UI 库的 17 个原因，而是将其定位为一个通过 UI 和 API 与机器学习模型交互的框架。Gradio 提供了诸如通用 API 访问的功能，通过官方 Python (`gradio_client`) 和 JavaScript (`@gradio/client`) 的 SDK，以及对 cURL API 访问的支持，并自动生成 REST API 端点。其他关键特性包括：交互式 API 记录器、用于快速 ML 应用的服务器端渲染、自动队列管理、用于实时输出的高性能流式传输、集成的多页面应用程序支持、使用 Groovy 的客户端函数执行、全面的主题系统、动态界面、使用 Gradio Sketch 的可视化界面开发、PWA 支持、使用 Gradio Lite 的浏览器内执行、AI 辅助工具、轻松的应用程序共享、企业级安全性、增强的 Dataframe 组件以及用于共享应用程序状态的深度链接。文章强调了 Gradio 对性能、安全性和响应性的关注，旨在构建强大的 AI 应用程序。

## Gradio App Features摘要 (Summary of Gradio App Features)

**中文:**

Gradio应用程序提供以下关键特性：

1.  **自动生成的API文档：** 通过“View API”链接访问，方便用户了解API接口。
2.  **客户端库：** 包含高级功能，如文件处理和Hugging Face Space复制，方便用户通过编程方式与Gradio应用交互。
3.  **交互式API录制器：** 从Gradio 4.26版本开始引入，可以实时捕获UI交互，并自动生成Python或JavaScript代码，用于记录API的使用方法。
4.  **服务器端渲染 (SSR)：** 从Gradio 5.0版本开始引入，显著提升了应用程序的加载速度和性能。SSR在服务器端预渲染UI，消除了加载动画，实现了更快的初始页面加载，并改善了已发布应用的SEO。SSR会自动在Hugging Face Spaces部署中启用。

**Gradio的优势:**

*   大多数其他Python框架缺少官方API访问机制。Gradio从单一实现自动生成UI和API端点，包括文档。
*   API录制器可以轻松脚本化UI交互，这在大多数其他Python和Web框架中难以实现。
*   传统的Python UI框架仅限于客户端渲染。

**英文:**

Gradio apps offer the following key features:

1.  **Automatically-generated API documentation:** Accessible through the "View API" link, providing easy access to API interface information.
2.  **Client libraries:** Include advanced features like file handling and Hugging Face Space duplication, facilitating programmatic interaction with Gradio apps.
3.  **Interactive API Recorder:** Introduced in Gradio version 4.26, allowing real-time capture of UI interactions and automatic generation of Python or JavaScript code for documenting API usage.
4.  **Server-Side Rendering (SSR):** Introduced in Gradio version 5.0, significantly improving application loading speed and performance. SSR pre-renders the UI on the server, eliminating loading spinners, enabling faster initial page load times, and improving SEO for published applications. SSR is automatically enabled for Hugging Face Spaces deployments.

**What Sets Gradio Apart:**

*   Most other Python frameworks lack official API access mechanisms. Gradio automatically generates both UI and API endpoints from a single implementation, including documentation.
*   The API Recorder allows easy scripting of UI interactions, which is difficult to achieve in most other Python and Web frameworks.
*   Traditional Python UI frameworks are limited to client-side rendering.

Okay, here's a summary of the provided text, with both English and Chinese versions, preserving formulas and terminology:

**English Summary:**

Gradio simplifies building high-performance ML web applications by providing features like automatic queue management and high-performance streaming. Unlike other Python frameworks, Gradio offers built-in queue management, eliminating the need for manual implementation of queuing systems that would be required for popular web frameworks and allowing for the creation of GPU-intensive or viral ML applications. Gradio's queuing system automatically handles diverse tasks (GPU-intensive computations, streaming, non-ML tasks), enables scaling to thousands of concurrent users, provides real-time queue status updates via Server-Side Events, and allows configuring concurrency limits using `concurrency_id`. Furthermore, Gradio supports real-time, low-latency streaming using Python generators with `yield` statements, enabling token-by-token text generation, step-by-step image generation, and smooth audio/video streaming via HTTP Live Streaming (HLS) protocol, and provides WebRTC/WebSocket API for real-time applications via `FastRTC`.  This is all achieved while maintaining a pure Python development experience, although Node.js installation is required.

**Chinese Summary:**

Gradio通过提供自动队列管理和高性能流式传输等功能，简化了高性能机器学习Web应用程序的构建。与其他Python框架不同，Gradio提供了内置的队列管理，无需像使用流行的Web框架那样手动实现队列系统，从而可以创建GPU密集型或病毒式ML应用程序。Gradio的队列系统自动处理各种任务（GPU密集型计算、流式传输、非ML任务），支持扩展到数千并发用户，通过Server-Side Events提供实时队列状态更新，并允许使用`concurrency_id`配置并发限制。此外，Gradio支持使用带有`yield`语句的Python生成器进行实时、低延迟流式传输，从而实现token-by-token文本生成、step-by-step图像生成以及通过HTTP Live Streaming (HLS) 协议实现流畅的音频/视频流，并通过`FastRTC`提供用于实时应用程序的WebRTC/WebSocket API。所有这些都在保持纯Python开发体验的同时实现，尽管需要安装Node.js。

好的，这里是该内容的摘要，包含中英文版本，并保留了公式和术语：

**摘要 (中文)**

Gradio 5 的改进包括：

*   **流式传输增强:** Gradio 允许使用 FastRTC 和 Gradio 完全使用 Python 创建实时音频/视频流应用程序，而其他框架需要手动线程管理和轮询更新，或需要自定义 WebSocket/WebRTC 实现。
*   **集成多页面应用支持:** Gradio 现在原生支持多页面应用，允许开发者构建更全面的 AI/ML 应用。它提供自动 URL 路由和导航栏生成，并在页面之间共享后端资源（如队列）。这提高了文件可维护性和测试性。Gradio 通过简单的 Python 声明提供自动路由和导航栏，无需像其他框架那样为每个页面编写单独的脚本或显式设置路由。
*   **使用 Groovy 的新客户端函数执行:** Gradio 5 引入了 Groovy，一个自动 Python 到 JavaScript 的转换库，允许在客户端直接执行 Python 函数 (通过 `js=True` 标记)，无需服务器往返即可实现即时 UI 响应。这减少了简单 UI 交互的延迟，并降低了服务器负载。

**摘要 (English)**

Gradio 5 improvements include:

*   **Streaming Improvements:** Gradio allows creating real-time audio/video streaming applications entirely in Python with FastRTC and Gradio. Other frameworks require manual thread management and polling for streaming updates, or custom WebSocket/WebRTC implementations.
*   **Integrated Multi-Page Application Support:** Gradio now natively supports multi-page applications, enabling developers to build more comprehensive AI/ML applications. It provides automatic URL routing and navigation bar generation and shares backend resources (such as the queue) across pages. This improves file maintainability and testing. Gradio offers automatic routing and navigation bars using simple Python declarations, eliminating the need for separate scripts for each page or explicit routing setup like other frameworks.
*   **New Client-Side Function Execution With Groovy:** Gradio 5 introduces Groovy, an automatic Python-to-JavaScript transpilation library, allowing direct execution of Python functions on the client-side (via the `js=True` flag) for instant UI responsiveness without server roundtrips. This reduces latency for simple UI interactions and lowers server load.

## 摘要 (Summary)

**英文 (English):**

Gradio distinguishes itself by offering a single-language (Python) development experience through automatic transpilation to JavaScript, enabling web-native performance without requiring separate JavaScript codebases, unlike most other Python frameworks.  It features a comprehensive theming system with ready-to-use presets (Monochrome, Soft, Ocean, Glass, etc.) including built-in dark mode support, automatic mobile responsiveness, and accessibility features for screen readers. Gradio provides ML-specific UI components such as Undo/Retry/Like buttons for chat interfaces, ImageEditor and AnnotatedImage components for segmentation/masking, and ImageSlider for image-to-image transformations. Recent enhancements focus on UI features for Reasoning LLMs, Agents, Multistep Agents, Nested Thoughts, and Nested Agents within chat interfaces. Furthermore, Gradio allows creating professional UIs without web design expertise and offers a dynamic interface feature using the `@gr.render()` decorator. This sets Gradio apart from other Python frameworks that offer limited color customization and require manual theme and CSS implementation.

**中文 (Chinese):**

Gradio 的独特之处在于，它通过自动转译为 JavaScript，提供了一种单语言 (Python) 开发体验，从而实现了 web 原生性能，无需像大多数其他 Python 框架那样使用单独的 JavaScript 代码库。 它具有全面的主题系统，提供即用型预设主题（例如 Monochrome、Soft、Ocean、Glass 等），包括内置的暗黑模式支持、自动的移动响应能力和屏幕阅读器的可访问性功能。 Gradio 提供特定于 ML 的 UI 组件，例如聊天界面的撤消/重做/喜欢按钮、用于分割/掩码的 ImageEditor 和 AnnotatedImage 组件，以及用于图像到图像转换的 ImageSlider。 最近的增强功能侧重于聊天界面中推理 LLM、Agent、多步 Agent、嵌套思维和嵌套 Agent 的 UI 功能。 此外，Gradio 允许在没有 Web 设计专业知识的情况下创建专业的 UI，并使用 `@gr.render()` 装饰器提供动态界面功能。 这使得 Gradio 与其他提供有限颜色自定义并需要手动主题和 CSS 实现的 Python 框架区分开来。

**Key Terms & Formula (关键术语和公式):**

*   `@gr.render()`: Decorator for dynamic interfaces. (动态界面的装饰器)
*   UI Components (用户界面组件): Undo/Retry/Like buttons, ImageEditor, AnnotatedImage, ImageSlider. (撤消/重做/喜欢按钮、图像编辑器、注释图像、图像滑块)
*   Theming System (主题系统): Monochrome, Soft, Ocean, Glass. (单色、柔和、海洋、玻璃)
*   LLMs, Agents, Multistep Agents, Nested Thoughts, Nested Agents (LLM、Agent、多步 Agent、嵌套思维、嵌套 Agent)

## 摘要 (Summary):

**English:**

Gradio now supports dynamic UI updates and a visual development environment, significantly enhancing its capabilities. Developers can now add new components and event listeners dynamically based on user interaction and state, enabling on-the-fly UI modifications based on model outputs or workflow.  Gradio's `.render()` method allows rendering Blocks within other Blocks.  Furthermore, Gradio Sketch introduces a no-code visual interface development tool (WYSIWYG editor) for building Gradio applications. Users can visually add components, define events, attach functions, and generate the corresponding code automatically, including code for inference functions.  Finally, Gradio offers Progressive Web App (PWA) support.

**Key Features:**

*   **Dynamic UI Updates:**  Add components and listeners dynamically based on user interaction/state.
*   **`render()` method:** Render Blocks within other Blocks.
*   **Gradio Sketch:** No-code visual interface development (WYSIWYG editor) for building Gradio applications.
*   **PWA Support:** Gradio provides Progressive Web App capabilities.

**优势 (Advantages):**

*   **Dynamic UI:** Simplifies the creation of sophisticated and responsive interfaces using Python, unlike other frameworks that require JavaScript for interface updates.
*   **Gradio Sketch:** Reduces the learning curve for non-coders and accelerates application development.
*   **PWA:** Provides regular web pages.

---

**中文:**

Gradio现在支持动态UI更新和一个可视化开发环境，从而显著增强了其功能。开发者现在可以基于用户交互和状态动态地添加新组件和事件监听器，从而能够根据模型输出或工作流程进行即时UI修改。Gradio的 `.render()` 方法允许在一个Block中渲染另一个Block。此外，Gradio Sketch 引入了一个无代码的可视化界面开发工具（WYSIWYG编辑器），用于构建Gradio应用程序。用户可以可视化地添加组件、定义事件、附加函数，并自动生成相应的代码，包括推理函数的代码。 最后，Gradio提供渐进式Web应用（PWA）支持。

**关键特性：**

*   **动态 UI 更新:** 基于用户交互/状态动态添加组件和监听器。
*   **`render()` 方法:** 在另一个Block中渲染Blocks。
*   **Gradio Sketch:** 用于构建Gradio应用程序的无代码可视化界面开发（WYSIWYG编辑器）。
*   **PWA 支持:** Gradio提供渐进式Web应用功能。

**优势：**

*   **动态 UI:** 简化了使用Python创建复杂且响应式界面的过程，不同于其他需要JavaScript进行界面更新的框架。
*   **Gradio Sketch:** 降低了非编码人员的学习曲线，并加速了应用程序开发。
*   **PWA:** 提供常规网页。

## 摘要 (Summary):

**中文:**

Gradio 通过以下特性简化了机器学习 (ML) 应用的开发和部署：

*   **渐进式 Web 应用 (PWA) 支持:** Gradio 提供原生 PWA 支持，允许创建看似平台专属的可安装应用，无需额外配置即可用于移动和桌面平台。这扩展了用户访问范围，并能快速创建带有自定义图标的移动应用。
*   **浏览器内执行 (Gradio Lite):**  利用 Pyodide (WebAssembly)，Gradio Lite 实现了浏览器端的执行，允许使用客户端模型推理服务（如 Transformers.js 和 ONNX）构建 ML 演示。 优势包括增强的隐私（所有数据保留在用户浏览器中）、零服务器部署成本和离线模型推理能力。
*   **AI 辅助工具加速开发:** Gradio 引入了创新功能来加速 ML 应用开发周期，包括热重载功能、用于自然语言驱动的应用生成的 AI Playground，以及与 HuggingFace 和推理提供商的集成，可以用单行代码快速构建应用原型。

**英文:**

Gradio simplifies the development and deployment of Machine Learning (ML) applications through the following features:

*   **Progressive Web App (PWA) Support:** Gradio offers native PWA support, allowing the creation of installable platform-specific applications that appear so to the user, without the need for extra configurations for mobile and desktop platforms. This expands user access and enables rapid creation of mobile apps with custom icons.
*   **In-Browser Execution (Gradio Lite):** Utilizing Pyodide (WebAssembly), Gradio Lite enables browser-side execution, allowing the building of ML demos using client-side model inference services like Transformers.js and ONNX.  Benefits include enhanced privacy (all data stays in the user's browser), zero server costs for deployment, and offline-capable model inference.
*   **Accelerated Development with AI-Assisted Tooling:** Gradio introduces innovative features to accelerate the ML application development cycle, including a hot reload capability, AI Playground for natural language-driven app generation, and integrations with HuggingFace and Inference providers, enabling rapid prototyping of applications in a single line of code.

## 摘要 (Summary)

**中文：**

Gradio 可以通过 `gr.load()` 与兼容 OpenAI 的 API 接口一起使用。Gradio 的独特之处在于它提供了即时的 UI 反馈和 AI 辅助开发，从而实现了快速开发和修改机器学习 (ML) 应用。应用程序可以通过设置 `demo.launch(share=True)` 参数生成一个即时公共 URL，格式为 `xxxxx.gradio.live`，有效期为 1 周。此共享链接使用 Fast Reverse Proxy (FRP) 通过 Gradio 的共享服务器建立到本地运行应用程序的安全 TLS 通道。对于企业部署或需要自定义域名或额外安全措施的情况，可以托管自己的 FRP 服务器以避免 1 周的超时。与其他框架不同，Gradio 无需云部署和大量配置即可轻松共享应用程序，而其他 Web 框架则需要手动服务器设置和托管。

**英文：**

Gradio can be used with any API endpoint that is compatible with OpenAI, achievable with `gr.load()`. It stands out by offering instant UI feedback and AI-assisted development, enabling rapid creation and modification of Machine Learning (ML) applications. You can generate an instant public URL by simply setting the parameter `demo.launch(share=True)`, creating an address in the format `xxxxx.gradio.live` that lasts for 1 week. This share link creates a secure TLS tunnel to your locally-running app through Gradio's share server using Fast Reverse Proxy (FRP). For enterprise deployments or scenarios requiring custom domains or additional security, you can host your own FRP server to avoid the 1-week timeout. Unlike other frameworks, Gradio offers instant sharing from your local development environment without cloud deployment and lots of configuration, while other Web frameworks require manual server setup and hosting.

## 摘要 (Summary)

**英文 (English):**

Gradio offers immediate collaboration and demonstration capabilities through share links without requiring hosting or port forwarding, making it ideal for rapid prototyping and gathering feedback on machine learning apps.  Gradio has evolved into a production-ready framework with enterprise-grade security, including third-party security audits and vulnerability assessments.  Security enhancements include hardened file handling and upload controls, configurable security settings via environment variables such as `GRADIO_ALLOWED_PATHS` and `GRADIO_SSR_MODE`.  Unlike other Python web frameworks, Gradio provides specialized security for ML deployment, including protected file upload handling and sanitized model I/O processing.  The enhanced dataframe component features multi-cell selection, row numbers, column pinning, search/filter functions, static columns, and improved accessibility.

**中文 (Chinese):**

Gradio 通过分享链接提供即时协作和演示功能，无需托管或端口转发，非常适合快速原型设计和收集机器学习应用程序的反馈。Gradio 已经发展成为一个具有企业级安全性的、可用于生产环境的框架，包括来自第三方的安全审计和漏洞评估。安全增强功能包括加强的文件处理和上传控制，以及通过环境变量（例如 `GRADIO_ALLOWED_PATHS` 和 `GRADIO_SSR_MODE`）进行配置的安全设置。 与其他 Python Web 框架不同，Gradio 为 ML 部署提供专门的安全保障，包括受保护的文件上传处理和经过清理的模型 I/O 处理。 增强的 Dataframe 组件具有多单元格选择、行号、列固定、搜索/筛选功能、静态列以及改进的可访问性。

## 摘要 (Summary):

**English:**

Gradio has evolved into an AI-focused framework for building complete web applications in Python without web development expertise. Innovations in Gradio 4 and 5, like Python-to-JavaScript transpilation, built-in queuing for resource-intensive models, real-time audio-video streaming with FastRTC, and server-side rendering, differentiate it from other frameworks by providing capabilities that often require extensive implementation work.  Key features include:

*   **Deep Links:** Enabled by the `gr.DeepLinkButton` component, they allow users to capture and share the exact state of an application, working with any public Gradio app (hosted or using `share=True`). This simplifies sharing of generated output without additional implementation.

Gradio handles infrastructure concerns, allowing developers to focus on model development while delivering polished UIs. It supports both rapid prototyping and production deployment.

**中文:**

Gradio已发展成为一个以人工智能为中心的框架，可以使用 Python 构建完整的 Web 应用程序，而无需 Web 开发专业知识。Gradio 4 和 5 的创新，例如 Python 到 JavaScript 的转译、针对资源密集型模型的内置队列、使用 FastRTC 的实时音视频流和服务器端渲染，使其与其他框架区分开来，因为它们提供了通常需要大量实现工作才能实现的功能。关键特性包括：

*   **深度链接 (Deep Links):** 通过 `gr.DeepLinkButton` 组件启用，允许用户捕获和共享应用程序的精确状态，适用于任何公共 Gradio 应用程序（托管或使用 `share=True`）。 这简化了生成输出的共享，而无需额外的实施。

Gradio 处理基础设施问题，使开发人员能够专注于模型开发，同时提供完善的用户界面。 它支持快速原型设计和生产部署。

Here's a summary of the provided text, in both English and Chinese:

**English Summary:**

This text highlights the capabilities of Gradio, a tool that empowers a wide audience. It mentions a blog post celebrating reaching 1 million Gradio users (published April 4, 2025, by abidlabs) and introduces a new Gradio Dataframe feature (published March 24, 2025, by hmb). A community post reports an error encountered while using `torch.load()` in a Gradio space: `_pickle.UnpicklingError: invalid load key, 'v'`. The user is attempting to load a model and receives the error when unpickling. The space involves KokoroTTS for custom voices, and the user is asking about the correct `torch` version. Functionality for uploading images, audio, and video is also mentioned, along with a prompt to sign up or log in to comment.  The error stack trace contains: `torch.serialization.py`, `_pickle.UnpicklingError`, `torch.load(path, map_location='cuda', weights_only=False)['net'].items()`.

**Chinese Summary:**

这段文字强调了 Gradio 的功能，它是一个赋能广大用户的工具。 它提到了一篇庆祝达到 100 万 Gradio 用户的博客文章（由 abidlabs 于 2025 年 4 月 4 日发布），并介绍了一个新的 Gradio Dataframe 功能（由 hmb 于 2025 年 3 月 24 日发布）。 一篇社区帖子报告了在使用 Gradio space 中的 `torch.load()` 时遇到的错误：`_pickle.UnpicklingError: invalid load key, 'v'`。 用户尝试加载模型时，在反序列化时收到了错误。 该 space 涉及用于自定义语音的 KokoroTTS，用户正在询问正确的 `torch` 版本。还提到了上传图像、音频和视频的功能，以及注册或登录以发表评论的提示。 错误堆栈跟踪包含：`torch.serialization.py`，`_pickle.UnpicklingError`，`torch.load(path, map_location='cuda', weights_only=False)['net'].items()`。

---

## [Cohere on Hugging Face Inference Providers 🔥](https://huggingface.co/blog/inference-providers-cohere) （2025-04-16）

## Summary (摘要)

**English:**

This article announces that Cohere is now a supported Inference Provider on the Hugging Face Hub. Cohere is committed to providing secure AI solutions for enterprise use-cases, including Generative AI, Embeddings, and Ranking models. Cohere Labs supports fundamental research. Several Cohere models are now available for serverless inference, including:

*   `CohereLabs/c4ai-command-r-v01`
*   `CohereLabs/c4ai-command-r-plus`
*   `CohereLabs/c4ai-command-r-08-2024`
*   `CohereLabs/c4ai-command-r7b-12-2024`
*   `CohereLabs/c4ai-command-a-03-2025`
*   `CohereLabs/aya-expanse-8b`
*   `CohereLabs/aya-expanse-32b`
*   `CohereLabs/aya-vision-8b`
*   `CohereLabs/aya-vision-32b`

Specifically, `CohereLabs/c4ai-command-a-03-2025` is highlighted for its suitability for enterprises requiring fast, secure, and high-quality AI, featuring a 256k context length.

**中文:**

这篇文章宣布 Cohere 现在是 Hugging Face Hub 上支持的 Inference Provider (推理提供商)。Cohere 致力于为企业用例提供安全的 AI 解决方案，包括 Generative AI (生成式AI)、Embeddings (嵌入) 和 Ranking (排序) 模型。 Cohere Labs 支持基础研究。现在可以通过 Cohere 和 Inference Providers 进行以下 Cohere 模型的 Serverless Inference (无服务器推理)：

*   `CohereLabs/c4ai-command-r-v01`
*   `CohereLabs/c4ai-command-r-plus`
*   `CohereLabs/c4ai-command-r-08-2024`
*   `CohereLabs/c4ai-command-r7b-12-2024`
*   `CohereLabs/c4ai-command-a-03-2025`
*   `CohereLabs/aya-expanse-8b`
*   `CohereLabs/aya-expanse-32b`
*   `CohereLabs/aya-vision-8b`
*   `CohereLabs/aya-vision-32b`

特别提到 `CohereLabs/c4ai-command-a-03-2025`，它适用于需要快速、安全和高质量 AI 的企业，并具有 256k context length (上下文长度)。

## Summary in English and Chinese:

**English:**

This text highlights Cohere's advanced Language Model offerings, emphasizing their multilingual capabilities and advanced features.  Key models mentioned include:

*   **aya-expanse-32b:**  Focuses on state-of-the-art multilingual support (23 languages), leveraging recent research in multilingual pre-training. Features a 128K context length.
*   **c4ai-command-r7b-12-2024:**  Ideal for low-cost/low-latency use cases.  Offers 128K context length, multilingual support (23 languages), citation-verified **Retrieval-Augmented Generation (RAG)**, reasoning, tool use, and agentic behavior. An open-weight model with strong performance.
*   **aya-vision-32b:**  A 32-billion parameter vision-language model optimized for tasks such as **OCR**, captioning, visual reasoning, summarization, question answering, and code. Supports 23 languages.

The text also describes how to use Cohere models on the Hugging Face Hub via the website UI and client SDKs (specifically Python with `huggingface_hub`).  It provides links to the Cohere documentation page and a Colab notebook for examples.

**Chinese:**

该文本重点介绍了Cohere先进的语言模型产品，强调其多语言能力和高级功能。提到的关键模型包括：

*   **aya-expanse-32b:** 专注于最先进的多语言支持（23种语言），利用多语言预训练的最新研究。具有128K的上下文长度。
*   **c4ai-command-r7b-12-2024:**  适用于低成本/低延迟用例。提供 128K 的上下文长度，多语言支持（23 种语言），引用验证的**检索增强生成 (RAG)**，推理，工具使用和 Agentic 行为。一个具有强大性能的开放权重模型。
*   **aya-vision-32b:** 一个320亿参数的视觉语言模型，针对诸如 **OCR**、字幕、视觉推理、摘要、问答和代码等任务进行了优化。支持23种语言。

该文本还描述了如何在 Hugging Face Hub 上通过网站 UI 和客户端 SDK（特别是使用 `huggingface_hub` 的 Python）使用 Cohere 模型。 它提供了指向 Cohere 文档页面和 Colab notebook 的链接，以供参考示例。

## Summary (摘要):

This document describes how to use Cohere as an inference provider via the Hugging Face Hub. It details the necessary setup, including installing `huggingface_hub>=0.30.0` and using the `InferenceClient` with the `provider="cohere"` parameter and your Cohere API key.

**Key points:**

*   **Setup:** Install `huggingface_hub>=0.30.0`.
*   **`InferenceClient`:** Instantiate with `provider="cohere"` and your API key:

    ```python
    from huggingface_hub import InferenceClient

    client = InferenceClient(
        provider="cohere",
        api_key="xxxxxxxxxxxxxxxxxxxxxxxx",
    )
    ```

*   **Chat Completion:** Use `client.chat.completions.create()` to generate responses.  Example usage with `CohereLabs/c4ai-command-r7b-12-2024`:

    ```python
    completion = client.chat.completions.create(
        model="CohereLabs/c4ai-command-r7b-12-2024",
        messages=messages,
        temperature=0.7,
        max_tokens=512,
    )
    ```

*   **Multimodal Support (Aya Vision):** Shows how to send base64 encoded images to Cohere's Aya Vision model:

    ```python
    image_path = "img.jpg"
    with open(image_path, "rb") as f:
        base64_image = base64.b64encode(f.read()).decode("utf-8")
    image_url = f"data:image/jpeg;base64,{base64_image}"

    messages = [
            {
    "role": "user",
    "content": [
                    {"type": "text", "text": "What's in this image?"},
                    {"type": "image_url", "image_url": {"url": image_url}},
                ]
            }
    ]

    completion = client.chat.completions.create(
        model="CohereLabs/aya-vision-32b",
        messages=messages,
        temperature=0.7,
        max_tokens=512,
    )
    ```
*   **JavaScript Example:** Demonstrates using `@huggingface/inference` to call Cohere chat completion endpoints from JavaScript.
```javascript
import { HfInference } from "@huggingface/inference";
const client = new HfInference("xxxxxxxxxxxxxxxxxxxxxxxx");
const chatCompletion = await client.chatCompletion({
    model: "CohereLabs/c4ai-command-a-03-2025",
    messages: [{ role: "user", content: "How to mak"}],
});
```

---

## 中文摘要:

本文档描述了如何通过 Hugging Face Hub 将 Cohere 用作推理提供商。 它详细介绍了必要的设置，包括安装 `huggingface_hub>=0.30.0` 和使用带有 `provider="cohere"` 参数和您的 Cohere API 密钥的 `InferenceClient`。

**关键点：**

*   **设置：** 安装 `huggingface_hub>=0.30.0`。
*   **`InferenceClient`：** 使用 `provider="cohere"` 和您的 API 密钥进行实例化：

    ```python
    from huggingface_hub import InferenceClient

    client = InferenceClient(
        provider="cohere",
        api_key="xxxxxxxxxxxxxxxxxxxxxxxx",
    )
    ```

*   **聊天补全：** 使用 `client.chat.completions.create()` 生成响应。 使用 `CohereLabs/c4ai-command-r7b-12-2024` 的示例用法：

    ```python
    completion = client.chat.completions.create(
        model="CohereLabs/c4ai-command-r7b-12-2024",
        messages=messages,
        temperature=0.7,
        max_tokens=512,
    )
    ```

*   **多模态支持 (Aya Vision):** 展示了如何将 base64 编码的图像发送到 Cohere 的 Aya Vision 模型：

    ```python
    image_path = "img.jpg"
    with open(image_path, "rb") as f:
        base64_image = base64.b64encode(f.read()).decode("utf-8")
    image_url = f"data:image/jpeg;base64,{base64_image}"

    messages = [
            {
    "role": "user",
    "content": [
                    {"type": "text", "text": "What's in this image?"},
                    {"type": "image_url", "image_url": {"url": image_url}},
                ]
            }
    ]

    completion = client.chat.completions.create(
        model="CohereLabs/aya-vision-32b",
        messages=messages,
        temperature=0.7,
        max_tokens=512,
    )
    ```

*  **JavaScript 示例：**演示了使用 `@huggingface/inference` 从 JavaScript 调用 Cohere 聊天补全端点。
```javascript
import { HfInference } from "@huggingface/inference";
const client = new HfInference("xxxxxxxxxxxxxxxxxxxxxxxx");
const chatCompletion = await client.chatCompletion({
    model: "CohereLabs/c4ai-command-a-03-2025",
    messages: [{ role: "user", content: "How to mak"}],
});
```

## 摘要 (Summary)

**中文:**

本文展示了如何通过 OpenAI 客户端库，使用 Cohere 作为推理提供者来调用 Command R7B 模型。文章提供了使用 `OpenAI` 客户端进行聊天补全的代码示例，包括设置 `base_url` 和 `api_key`，以及传递包含用户消息 `messages` 的列表。还介绍了 Cohere 模型的工具使用功能 (Tool Use)，并展示了如何定义工具（例如 `get_flight_info`，用于获取两地之间的航班信息）以及如何在 `tool_calls` 中传递消息给推理客户端，以便模型在需要时使用这些工具。 工具的定义包括 `type`，`function` 和 `parameters`，其中参数包括 `loc_origin` 和 `loc_destination`，用于指定起飞地和目的地机场。

**English:**

This document demonstrates how to call the Command R7B model using Cohere as the inference provider via the OpenAI client library. It provides a code example of using the `OpenAI` client for chat completions, including setting the `base_url` and `api_key`, and passing a list of `messages` containing the user's message. It also introduces the Tool Use feature of Cohere models and shows how to define tools (e.g., `get_flight_info`, which retrieves flight information between two locations) and how to pass messages with `tool_calls` to the inference client so that the model can use these tools when relevant. The tool definition includes `type`, `function`, and `parameters`, where the parameters include `loc_origin` and `loc_destination` for specifying the departure and destination airports.

好的，这是对你提供的内容的摘要，包含中英双语，并保留了公式与术语：

**摘要：**

This text demonstrates how to use the `huggingface_hub` library and `InferenceClient` to interact with a large language model (LLM) from Cohere. It provides a code snippet that uses the `client.chat.completions.create` method to generate a response based on a series of messages and tools. The example shows a multi-turn conversation where a user asks about flight information, the assistant uses a tool (a function call with `name` "get_flight_info" and `arguments` `'{ "loc_destination": "Seattle", "loc_origin": "Miami" }'`) to retrieve the information, and the tool returns the flight details.  The code sets parameters like `model` ("CohereLabs/c4ai-command-r7b-12-2024"), `messages`, `tools`, `temperature` (0.7), and `max_tokens` (512). The document also outlines billing information for both direct requests (using a Cohere key) and routed requests (authenticating via the Hugging Face Hub).  PRO users get $2 worth of Inference credits per month.

**中文摘要：**

这段文字展示了如何使用 `huggingface_hub` 库和 `InferenceClient` 来与 Cohere 的大型语言模型 (LLM) 交互。它提供了一段代码，使用 `client.chat.completions.create` 方法基于一系列的消息和工具来生成回复。示例展示了一个多轮对话，其中用户询问航班信息，助手使用一个工具（一个函数调用， `name` 为 "get_flight_info"， `arguments` 为 `'{ "loc_destination": "Seattle", "loc_origin": "Miami" }'`）来检索信息，然后工具返回航班详情。代码设置了诸如 `model` ("CohereLabs/c4ai-command-r7b-12-2024")、`messages`、`tools`、`temperature` (0.7) 和 `max_tokens` (512) 等参数。该文档还概述了直接请求（使用 Cohere 密钥）和路由请求（通过 Hugging Face Hub 验证）的计费信息。PRO 用户每月可获得价值 2 美元的推理积分。

关键术语：`huggingface_hub`, `InferenceClient`, `client.chat.completions.create`, `messages`, `tools`, `temperature`, `max_tokens`, `model`, `function call`, `name`, `arguments`.

**摘要 (Summary):**

The user "borgr" inquired about the possibility of exporting or retrieving user conversations from the UI or via API, potentially for purposes such as data labeling, studying user needs ("what people lack"), and understanding human reactions to Large Language Model (LM) behavior. The article author, "merve," responded that such an option is not currently available but will be considered. They suggested using the providers programmatically and storing the data independently in the meantime.

**英文摘要 (English Summary):**

The user "borgr" asked if there was a way to export user conversations from the UI or retrieve them via an API request. Their use cases included data labeling, studying user needs ("what people lack"), and understanding human reactions to Large Language Model (LM) behavior. The article author, "merve," replied that this functionality is not yet available but will be considered. They suggested using the providers programmatically and storing the data independently for now.

---

## [Introducing HELMET](https://huggingface.co/blog/helmet) （2025-04-16）



---

