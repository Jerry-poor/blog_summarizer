# HF åšå®¢æ—¥æŠ¥ â€” 2025-04-16

ä»¥ä¸‹å…±æœ‰ 3 ç¯‡åšæ–‡ï¼Œæ‘˜è¦å¦‚ä¸‹ï¼š

## [17 Reasons Why Gradio Isn't Just Another UI Library](https://huggingface.co/blog/why-gradio-stands-out) ï¼ˆ2025-04-16ï¼‰

**Abstract:**

Gradio is more than a UI library; it's a framework for interacting with machine learning models.
*Gradio is more than a UI library; it's a framework for interacting with machine learning models.*

It offers Universal API Access, including Python (gradio_client) and JavaScript (@gradio/client) SDKs, and automatic REST API generation.
*It offers Universal API Access, including Python (gradio_client) and JavaScript (@gradio/client) SDKs, and automatic REST API generation.*

Features include interactive API recording, server-side rendering, automatic queue management, high-performance streaming, multi-page support, and client-side Groovy execution.
*Features include interactive API recording, server-side rendering, automatic queue management, high-performance streaming, multi-page support, and client-side Groovy execution.*

Gradio provides a comprehensive theming system, dynamic interfaces, visual development with Sketch, PWA support, in-browser execution with Lite, AI-assisted tooling, hassle-free sharing, enterprise-grade security, enhanced Dataframe component, and Deep Links.
*Gradio provides a comprehensive theming system, dynamic interfaces, visual development with Sketch, PWA support, in-browser execution with Lite, AI-assisted tooling, hassle-free sharing, enterprise-grade security, enhanced Dataframe component, and Deep Links.*

These features enable building powerful and secure AI applications with performance guarantees.
*These features enable building powerful and secure AI applications with performance guarantees.*

## Abstract

**æ‘˜è¦**

åœ¨JS Webæ¡†æ¶ä¸­å®ç°SSRæ¸²æŸ“éœ€è¦å¹¿æ³›çš„å…¨æ ˆå¼€å‘çŸ¥è¯†ã€‚
*Rendering while implementing SSR in JS web frameworks requires extensive full-stack development expertise.*

Gradioåœ¨ä¿æŒçº¯Pythonå¼€å‘ä½“éªŒçš„åŒæ—¶ï¼Œæä¾›äº†Webæ¡†æ¶çº§åˆ«çš„æ€§èƒ½ï¼ˆæ³¨æ„ï¼šé™¤äº†éœ€è¦å®‰è£…Nodeï¼ï¼‰ã€‚
*Gradio delivers web framework-level performance while maintaining a pure Python development experience (Note: except for having to install Node!).*

Gradioä¸ºMLåº”ç”¨æä¾›äº†ä¸€ä¸ªå¤æ‚çš„é˜Ÿåˆ—ç³»ç»Ÿï¼Œå¤„ç†GPUå¯†é›†å‹è®¡ç®—å’Œé«˜æµé‡ç”¨æˆ·è®¿é—®ã€‚
*Gradio provides a sophisticated queuing system tailored for ML applications that handles both GPU-intensive computations and high-volume user access.*

Gradioçš„é˜Ÿåˆ—è‡ªåŠ¨å¤„ç†åº”ç”¨ä¸­å®šä¹‰çš„ä¸åŒç±»å‹çš„ä»»åŠ¡ï¼Œæ— è®ºæ˜¯GPUä¸Šè¿è¡Œçš„é•¿é¢„æµ‹ï¼ŒéŸ³é¢‘/è§†é¢‘æµï¼Œè¿˜æ˜¯éMLä»»åŠ¡ã€‚
*Gradio's queue automatically handles different kinds of tasks defined in your application, whether they are long predictions that run on a GPU, audio/video streaming, or non-ML tasks.*

æ‚¨çš„åº”ç”¨å¯ä»¥æ‰©å±•åˆ°æ•°åƒå¹¶å‘ç”¨æˆ·ï¼Œè€Œä¸ä¼šå‡ºç°èµ„æºäº‰ç”¨å’Œç³»ç»Ÿè¿‡è½½ã€‚
*Your applications can scale to thousands of concurrent users without resource contention and system overwhelming.*

é€šè¿‡æœåŠ¡å™¨å‘é€äº‹ä»¶ (Server-Side Events) æä¾›å®æ—¶é˜Ÿåˆ—çŠ¶æ€æ›´æ–°ï¼Œå‘ç”¨æˆ·å±•ç¤ºä»–ä»¬åœ¨é˜Ÿåˆ—ä¸­çš„å½“å‰ä½ç½®ã€‚
*Real-time queue status updates via Server-Side Events, showing users their current position in the queue.*

Gradioçš„æµå¼ä¼ è¾“èƒ½åŠ›é€šè¿‡Pythonç”Ÿæˆå™¨å’Œ`yield`è¯­å¥å®ç°ï¼Œé€šè¿‡ HTTP Live Streaming (HLS) åè®®æ”¯æŒå®æ—¶ã€ä½å»¶è¿Ÿçš„æ›´æ–°ï¼Œç”¨äºtoken-by-tokenæ–‡æœ¬ç”Ÿæˆã€å›¾åƒç”Ÿæˆæ›´æ–°ï¼Œæˆ–éŸ³é¢‘/è§†é¢‘æµã€‚
*Gradio's streaming capabilities, implemented via Python generators and `yield` statements, support real-time, low-latency updates via HTTP Live Streaming (HLS) protocol for token-by-token text generation, image generation updates, or audio/video streaming.*

```
**Abstract:**

Gradio 5 significantly enhances real-time streaming capabilities with FastRTC, enabling Python-based audio/video streaming applications.
Gradio 5 significantly enhances real-time streaming capabilities with FastRTC, enabling Python-based audio/video streaming applications.

Unlike other frameworks requiring manual thread management, Gradio offers seamless streaming.
Unlike other frameworks requiring manual thread management, Gradio offers seamless streaming.

Native multi-page support allows for building comprehensive AI/ML applications, featuring automatic URL routing.
Native multi-page support allows for building comprehensive AI/ML applications, featuring automatic URL routing.

Backend resources are shared across pages, improving file maintainability.
Backend resources are shared across pages, improving file maintainability.

Gradio simplifies routing compared to other frameworks requiring explicit setup.
Gradio simplifies routing compared to other frameworks requiring explicit setup.

Groovy introduces Python-to-JavaScript transpilation for instant UI responsiveness via `js=True`.
Groovy introduces Python-to-JavaScript transpilation for instant UI responsiveness via `js=True`.

This reduces server load and latency, particularly beneficial for high-traffic applications.
This reduces server load and latency, particularly beneficial for high-traffic applications.
```

**Abstract**

Gradio provides a single-language Python development experience, automatically transpiling to JavaScript for web-native performance, unlike frameworks requiring separate JavaScript codebases.
Gradio æä¾›å•ä¸€è¯­è¨€çš„ Python å¼€å‘ä½“éªŒï¼Œè‡ªåŠ¨è½¬è¯‘ä¸º JavaScript ä»¥å®ç° Web åŸç”Ÿæ€§èƒ½ï¼Œä¸åŒäºéœ€è¦å•ç‹¬ JavaScript ä»£ç åº“çš„æ¡†æ¶ã€‚

Its comprehensive theming system and ready-to-use theme presets (Monochrome, Soft, Ocean, Glass) offer professional UI, including built-in dark mode and mobile responsiveness.
å…¶å…¨é¢çš„ä¸»é¢˜ç³»ç»Ÿå’Œå³ç”¨å‹ä¸»é¢˜é¢„è®¾ï¼ˆMonochromeã€Softã€Oceanã€Glassï¼‰æä¾›ä¸“ä¸šçš„ UIï¼ŒåŒ…æ‹¬å†…ç½®çš„æš—é»‘æ¨¡å¼å’Œç§»åŠ¨å“åº”èƒ½åŠ›ã€‚

ML-specific UI components (ImageEditor, ImageSlider) and enhanced features for LLMs and Agents are included.
åŒ…å« ML ç‰¹å®šçš„ UI ç»„ä»¶ï¼ˆImageEditorï¼ŒImageSliderï¼‰ä»¥åŠ LLM å’Œ Agent çš„å¢å¼ºåŠŸèƒ½ã€‚

Gradio's `@gr.render()` facilitates dynamic interfaces.
Gradio çš„ `@gr.render()` æœ‰åŠ©äºåŠ¨æ€ç•Œé¢ã€‚

It enables ML practitioners to create polished applications without web design expertise.
å®ƒä½¿ ML ä»ä¸šè€…æ— éœ€ Web è®¾è®¡ä¸“ä¸šçŸ¥è¯†å³å¯åˆ›å»ºç²¾ç¾çš„åº”ç”¨ç¨‹åºã€‚

Other frameworks offer limited color customization requiring manual CSS implementation.
å…¶ä»–æ¡†æ¶æä¾›çš„é¢œè‰²è‡ªå®šä¹‰åŠŸèƒ½æœ‰é™ï¼Œéœ€è¦æ‰‹åŠ¨å®ç° CSSã€‚

**Abstract:**

Gradio now supports dynamic UI modifications based on user interaction and state.
*Gradio now supports dynamic UI modifications based on user interaction and state.*

Developers can render UI on-the-fly based on model outputs, utilizing the `.render()` method for rendering Blocks within Blocks.
*Developers can render UI on-the-fly based on model outputs, utilizing the `.render()` method for rendering Blocks within Blocks.*

Gradio Sketch introduces a WYSIWYG editor for no-code ML application design, enabling visual layout creation, event definition, and automatic code generation.
*Gradio Sketch introduces a WYSIWYG editor for no-code ML application design, enabling visual layout creation, event definition, and automatic code generation.*

Compared to other frameworks requiring JavaScript or manual coding for UI updates, Gradio simplifies dynamic UI manipulation with Python and democratizes AI development.
*Compared to other frameworks requiring JavaScript or manual coding for UI updates, Gradio simplifies dynamic UI manipulation with Python and democratizes AI development.*

Gradio also offers Progressive Web App (PWA) support.
*Gradio also offers Progressive Web App (PWA) support.*

```
**Abstract:**

Gradio facilitates the creation of Progressive Web Apps (PWAs) for mobile and desktop, eliminating the need for extra configurations.
Gradio å¯ä»¥è½»æ¾åˆ›å»ºé€‚ç”¨äºç§»åŠ¨å’Œæ¡Œé¢å¹³å°çš„æ¸è¿›å¼ Web åº”ç”¨ (PWAs)ï¼Œæ— éœ€é¢å¤–é…ç½®ã€‚
This capability expands user access, enabling instant mobile app creation.
æ­¤åŠŸèƒ½æ‰©å±•äº†ç”¨æˆ·è®¿é—®èŒƒå›´ï¼Œèƒ½å¤Ÿå³æ—¶åˆ›å»ºç§»åŠ¨åº”ç”¨ç¨‹åºã€‚
Gradio Lite supports in-browser execution via Pyodide (WebAssembly), enabling client-side model inference using Transformers.js and ONNX, enhancing privacy and eliminating server costs.
Gradio Lite æ”¯æŒé€šè¿‡ Pyodide (WebAssembly) åœ¨æµè§ˆå™¨ç«¯æ‰§è¡Œï¼Œå¯ä»¥ä½¿ç”¨ Transformers.js å’Œ ONNX è¿›è¡Œå®¢æˆ·ç«¯æ¨¡å‹æ¨ç†ï¼Œä»è€Œå¢å¼ºéšç§å¹¶æ¶ˆé™¤æœåŠ¡å™¨æˆæœ¬ã€‚
Unlike other frameworks, Gradio allows serverless deployment of Python ML applications, even on static hosting services.
ä¸å…¶ä»–æ¡†æ¶ä¸åŒï¼ŒGradio å…è®¸ Python æœºå™¨å­¦ä¹ åº”ç”¨ç¨‹åºçš„æ— æœåŠ¡å™¨éƒ¨ç½²ï¼Œå³ä½¿åœ¨é™æ€æ‰˜ç®¡æœåŠ¡ä¸Šä¹Ÿæ˜¯å¦‚æ­¤ã€‚
Gradio accelerates development with features like hot reload and AI Playground for natural language-driven app generation, supporting rapid prototyping with `HuggingFace` integrations.
Gradio é€šè¿‡çƒ­é‡è½½å’Œç”¨äºè‡ªç„¶è¯­è¨€é©±åŠ¨åº”ç”¨ç¨‹åºç”Ÿæˆçš„ AI Playground ç­‰åŠŸèƒ½åŠ é€Ÿå¼€å‘ï¼Œæ”¯æŒä½¿ç”¨ `HuggingFace` é›†æˆè¿›è¡Œå¿«é€ŸåŸå‹è®¾è®¡ã€‚
```

Absolutely! Here's the abstract, adhering to your specifications:

**Abstract**

Gradio facilitates rapid ML application development using `gr.load()` and OpenAI-compatible API endpoints.
_Gradio facilitates rapid ML application development using `gr.load()` and OpenAI-compatible API endpoints._

It offers instant UI feedback and AI-assisted tooling, enabling researchers and developers to quickly iterate.
_It offers instant UI feedback and AI-assisted tooling, enabling researchers and developers to quickly iterate._

Unlike other frameworks requiring manual refreshes or complex build pipelines, Gradio provides immediate code update reflection.
_Unlike other frameworks requiring manual refreshes or complex build pipelines, Gradio provides immediate code update reflection._

Hassle-free app sharing is achieved via `demo.launch(share=True)`, generating a public URL on the `*.gradio.live` domain for one week, tunneled securely via Fast Reverse Proxy (FRP).
_Hassle-free app sharing is achieved via `demo.launch(share=True)`, generating a public URL on the `*.gradio.live` domain for one week, tunneled securely via Fast Reverse Proxy (FRP)._

For extended availability and custom domains, self-hosting an FRP server is an option, bypassing the default 168-hour timeout.
_For extended availability and custom domains, self-hosting an FRP server is an option, bypassing the default 168-hour timeout._

**Word Count:** 120 words

---

## [Cohere on Hugging Face Inference Providers ğŸ”¥](https://huggingface.co/blog/inference-providers-cohere) ï¼ˆ2025-04-16ï¼‰

**Abstract:**

We announce Cohere's integration as a supported Inference Provider on Hugging Face Hub.
æˆ‘ä»¬å®£å¸ƒ Cohere å·²é›†æˆåˆ° Hugging Face Hub ä½œä¸ºæ”¯æŒçš„æ¨ç†æä¾›å•†ã€‚

This marks the first model creator to directly share and serve models on the Hub.
è¿™æ ‡å¿—ç€ç¬¬ä¸€ä¸ªæ¨¡å‹åˆ›å»ºè€…ç›´æ¥åœ¨ Hub ä¸Šå…±äº«å’Œæä¾›æ¨¡å‹ã€‚

Cohere offers secure AI solutions for enterprise use, including Generative AI, Embeddings, and Ranking models.
Cohere ä¸ºä¼ä¸šç”¨é€”æä¾›å®‰å…¨çš„ AI è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ç”Ÿæˆå¼ AIã€åµŒå…¥å’Œæ’åºæ¨¡å‹ã€‚

CohereLabs supports fundamental research and innovation.
CohereLabs æ”¯æŒåŸºç¡€ç ”ç©¶å’Œåˆ›æ–°ã€‚

Users can now run serverless inference on models like *CohereLabs/c4ai-command-a-03-2025* and *CohereLabs/aya-expanse-32b*.
ç”¨æˆ·ç°åœ¨å¯ä»¥åœ¨*CohereLabs/c4ai-command-a-03-2025*å’Œ*CohereLabs/aya-expanse-32b*ç­‰æ¨¡å‹ä¸Šè¿è¡Œæ— æœåŠ¡å™¨æ¨ç†ã€‚

*CohereLabs/c4ai-command-a-03-2025* is optimized for enterprises needing fast, secure, high-quality AI with a 256k context length.
*CohereLabs/c4ai-command-a-03-2025* é’ˆå¯¹éœ€è¦å¿«é€Ÿã€å®‰å…¨ã€é«˜è´¨é‡ AI ä¸”ä¸Šä¸‹æ–‡é•¿åº¦ä¸º 256k çš„ä¼ä¸šè¿›è¡Œäº†ä¼˜åŒ–ã€‚

Leverage Cohere and Cohere Labs for advanced AI applications.
åˆ©ç”¨ Cohere å’Œ Cohere Labs å®ç°é«˜çº§ AI åº”ç”¨ç¨‹åºã€‚

**æ‘˜è¦ (Abstract)**

æœ¬æ–‡ä»‹ç»Cohereçš„å…ˆè¿›æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æ¨¡å‹ï¼Œå…·æœ‰å¯éªŒè¯çš„å¼•æ–‡ã€æ™ºèƒ½ä½“å·¥å…·ä½¿ç”¨ã€ä¼ä¸šçº§å®‰å…¨æ€§å’Œå¼ºå¤§çš„å¤šè¯­è¨€æ€§èƒ½ï¼ˆæ”¯æŒ 23 ç§è¯­è¨€ï¼‰ã€‚
This paper introduces Cohere's advanced retrieval-augmented generation (RAG) models with verifiable citations, agentic tool use, enterprise-grade security, and strong multilingual performance (support for 23 languages).

CohereLabs/aya-expanse-32b ä¸“æ³¨äºæœ€å…ˆè¿›çš„å¤šè¯­è¨€æ”¯æŒï¼Œåº”ç”¨äº†æœ€æ–°çš„å¤šè¯­è¨€é¢„è®­ç»ƒç ”ç©¶ã€‚
CohereLabs/aya-expanse-32b focuses on state-of-the-art multilingual support, applying the latest research on multilingual pre-training.

CohereLabs/c4ai-command-r7b-12-2024 é€‚ç”¨äºä½æˆæœ¬æˆ–ä½å»¶è¿Ÿç”¨ä¾‹ï¼Œåœ¨åŒç±»å¼€æºæ¨¡å‹ä¸­æä¾›æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰ 128K çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚
CohereLabs/c4ai-command-r7b-12-2024 is ideal for low-cost or low-latency use cases, bringing state-of-the-art performance in its class of open-weight models, with a context length of 128K.

CohereLabs/aya-vision-32b æ˜¯ä¸€ä¸ª 320 äº¿å‚æ•°æ¨¡å‹ï¼Œé’ˆå¯¹å„ç§è§†è§‰-è¯­è¨€ç”¨ä¾‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæ‰©å±•äº†å¤šæ¨¡æ€åŠŸèƒ½è‡³ 23 ç§è¯­è¨€ã€‚
CohereLabs/aya-vision-32b is a 32-billion parameter model optimized for vision-language use cases, expanding multimodal capabilities to 23 languages.

å¯ä»¥é€šè¿‡ Hugging Face Hub çš„ç½‘é¡µ UI æˆ–å®¢æˆ·ç«¯ SDK ç›´æ¥ä½¿ç”¨ Cohere æ¨¡å‹ã€‚
Cohere models can be directly used on the Hub either on the website UI or via the client SDKs.

**Abstract:**

æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ Hugging Face Hub ä½œä¸ºæ¥å£ï¼Œè°ƒç”¨ Cohere çš„æ¨ç†æœåŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆå’Œå¤šæ¨¡æ€å¤„ç†ã€‚
This paper introduces how to utilize Hugging Face Hub as an interface to invoke Cohere's inference services, including text generation and multimodal processing.

é€šè¿‡è®¾ç½® `provider="cohere"` å’Œ API å¯†é’¥ï¼Œå¯ä»¥è½»æ¾åœ°è°ƒç”¨ Cohere çš„ç«¯ç‚¹ã€‚
By setting `provider="cohere"` and the API key, Cohere's endpoints can be easily called.

æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `InferenceClient` å’Œ `chat.completions.create()` æ–¹æ³•ï¼Œä½¿ç”¨å¦‚ "CohereLabs/c4ai-command-r7b-12-2024" ç­‰æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚
The text generation example demonstrates how to use the `InferenceClient` and `chat.completions.create()` methods, using models like "CohereLabs/c4ai-command-r7b-12-2024" for dialogue.

å¤šæ¨¡æ€å¤„ç†ç¤ºä¾‹åˆ™å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ base64 ç¼–ç åµŒå…¥å›¾åƒï¼Œå¹¶ä½¿ç”¨ "CohereLabs/aya-vision-32b" ç­‰æ¨¡å‹è¿›è¡Œå›¾åƒç†è§£ï¼Œåˆ©ç”¨ `image_url = f"data:image/jpeg;base64,{base64_image}"` ä¼ é€’å›¾åƒæ•°æ®ã€‚
The multimodal processing example demonstrates how to embed images via base64 encoding and utilize models such as "CohereLabs/aya-vision-32b" for image understanding, passing image data using `image_url = f"data:image/jpeg;base64,{base64_image}"`.

**Abstract**

This work demonstrates calling Cohere's Command R7B model using the OpenAI client library via inference providers.
æœ¬ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡Inference Providersä½¿ç”¨OpenAIå®¢æˆ·ç«¯åº“è°ƒç”¨Cohereçš„Command R7Bæ¨¡å‹ã€‚

The implementation utilizes a custom base URL (e.g., `"https://router.huggingface.co/cohere/compatibility/v1"`) and API key for authentication.
è¯¥å®ç°ä½¿ç”¨è‡ªå®šä¹‰çš„ base URL (ä¾‹å¦‚, `"https://router.huggingface.co/cohere/compatibility/v1"`) å’Œ API å¯†é’¥è¿›è¡Œèº«ä»½éªŒè¯ã€‚

Furthermore, we explore agentic tool use, defining a `get_flight_info` function with parameters for origin (`loc_origin`) and destination (`loc_destination`) airports.
æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†agenticå·¥å…·çš„ä½¿ç”¨ï¼Œå®šä¹‰äº†ä¸€ä¸ª `get_flight_info` å‡½æ•°ï¼Œå…¶å‚æ•°åŒ…æ‹¬èµ·å§‹æœºåœº(`loc_origin`)å’Œç›®æ ‡æœºåœº(`loc_destination`)ã€‚

The tool definition is then incorporated into the model's chat template, enabling the model to make appropriate `tool_calls` during inference.
ç„¶åï¼Œå·¥å…·å®šä¹‰è¢«æ•´åˆåˆ°æ¨¡å‹çš„èŠå¤©æ¨¡æ¿ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œé€‚å½“çš„ `tool_calls`ã€‚

**Abstract:**

This work demonstrates the integration of a large language model (LLM) for flight information retrieval using the Hugging Face InferenceClient. This work utilizes a Cohere model `CohereLabs/c4ai-command-r7b-12-2024`.
æœ¬æ–‡å±•ç¤ºäº†ä½¿ç”¨Hugging Face InferenceClienté›†æˆå¤§å‹è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œèˆªç­ä¿¡æ¯æ£€ç´¢ã€‚This work utilizes a Cohere model `CohereLabs/c4ai-command-r7b-12-2024`.

The system processes a multi-turn conversation, including developer's date info, user's flight query (Miami to Seattle), and assistant's `tool_calls` using a function `get_flight_info` with arguments like `{ "loc_destination": "Seattle", "loc_origin": "Miami" }`.
è¯¥ç³»ç»Ÿå¤„ç†å¤šè½®å¯¹è¯ï¼ŒåŒ…æ‹¬å¼€å‘äººå‘˜çš„æ—¥æœŸä¿¡æ¯ã€ç”¨æˆ·çš„èˆªç­æŸ¥è¯¢ï¼ˆè¿ˆé˜¿å¯†åˆ°è¥¿é›…å›¾ï¼‰ä»¥åŠåŠ©æ‰‹ä½¿ç”¨å‡½æ•°`get_flight_info`çš„`tool_calls`ï¼Œå‚æ•°å¦‚`{ "loc_destination": "Seattle", "loc_origin": "Miami" }`ã€‚

The tool provides the flight information, `Miami to Seattle, May 1st, 10 AM.`, which returns to the LLM.
è¯¥å·¥å…·æä¾›äº†èˆªç­ä¿¡æ¯ï¼Œ`Miami to Seattle, May 1st, 10 AM.`ï¼Œè¿”å›ç»™LLMã€‚

Billing information is provided, emphasizing the standard Cohere API rates and potential Inference credits for PRO users.
æä¾›äº†è´¦å•ä¿¡æ¯ï¼Œå¼ºè°ƒäº†æ ‡å‡†çš„Cohere APIè´¹ç‡å’ŒPROç”¨æˆ·çš„æ½œåœ¨Inference creditsã€‚

The paper also mentions related articles and community feedback.
æœ¬æ–‡è¿˜æåŠäº†ç›¸å…³æ–‡ç« å’Œç¤¾åŒºåé¦ˆã€‚

**Abstract**

æœ¬æ–‡è®¨è®ºäº†ç”¨æˆ·å¯¹èŠå¤©è®°å½•APIè®¿é—®çš„éœ€æ±‚ï¼Œä»¥ä¾¿è¿›è¡Œæ•°æ®æ ‡æ³¨å’Œç”¨æˆ·è¡Œä¸ºç ”ç©¶ã€‚
This paper discusses the user demand for API access to chat logs for data labeling and user behavior research.

ç›®å‰ï¼Œè¯¥åŠŸèƒ½å°šæœªå®ç°ï¼Œä½†å·²è¢«å¼€å‘è€…è€ƒè™‘ã€‚
Currently, this functionality is not implemented but is under consideration by the developers.

ç”¨æˆ·å»ºè®®é€šè¿‡APIè¯·æ±‚è·å–å¯¹è¯æ•°æ®ï¼Œä»¥ä¾¿æ„å»ºè‡ªå®šä¹‰çš„æ•°æ®å¤„ç†æ–¹æ¡ˆã€‚
Users suggest obtaining conversation data via API requests to build custom data processing solutions.

ä¸€ç§æ›¿ä»£æ–¹æ¡ˆæ˜¯ç”¨æˆ·å¯ä»¥é€šè¿‡ç¼–ç¨‹æ–¹å¼ä½¿ç”¨providersï¼Œå¹¶å°†æ•°æ®è‡ªè¡Œå­˜å‚¨ã€‚
An alternative is for users to programmatically use providers and store the data themselves.

ç ”ç©¶ç›®çš„åŒ…æ‹¬æ ‡æ³¨ã€åˆ†æç”¨æˆ·éœ€æ±‚ä»¥åŠç ”ç©¶äººç±»å¯¹å„ç§è¯­è¨€æ¨¡å‹è¡Œä¸ºçš„ååº”ï¼Œå¦‚å­¦ä¹ äººç±»ååº” $R(LM)$ã€‚
Research purposes include labeling, analyzing user needs, and studying human reactions to various language model behavior, such as learning human reaction $R(LM)$.

---

## [Introducing HELMET](https://huggingface.co/blog/helmet) ï¼ˆ2025-04-16ï¼‰

**Abstract:**

è¯„ä¼°é•¿æ–‡æœ¬è¯­è¨€æ¨¡å‹ï¼ˆLCLMsï¼‰æ—¢å…·æœ‰æŒ‘æˆ˜æ€§åˆè‡³å…³é‡è¦ã€‚Evaluating long-context language models (LCLMs) is challenging but important. ç°æœ‰è¯„ä¼°è¿‡åº¦ä¾èµ–åˆæˆä»»åŠ¡ã€‚Existing evaluations overly rely on synthetic tasks. æˆ‘ä»¬å¼•å…¥HELMETï¼Œæ—¨åœ¨ä¸ºLCLMsæ„å»ºå¤šæ ·ã€å¯æ§å’Œå¯é çš„è¯„ä¼°ã€‚We introduce HELMET to craft diverse, controllable, and reliable evaluations for LCLMs. ç›¸è¾ƒäºç°æœ‰åŸºå‡†ï¼ŒHELMETè¿›è¡Œäº†å…³é”®æ”¹è¿›ã€‚HELMET includes key improvements over existing benchmarks. ç»“æœè¡¨æ˜ï¼ŒLCLMsåœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­ä»æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ï¼Œæ¨¡å‹æ€§èƒ½éšé•¿åº¦å’Œä»»åŠ¡å¤æ‚æ€§å¢åŠ è€Œä¸‹é™ã€‚Results show that LCLMs still have a long way to go on real-world tasks, and models degrade with increasing lengths and task complexity. HELMETä¿ƒè¿›å¯¹LCLMsèƒ½åŠ›çš„æ›´å…¨é¢è¯„ä¼°ï¼Œå¹¶æ¨åŠ¨æœªæ¥å‘å±•ã€‚HELMET facilitates a more comprehensive assessment of LCLM capabilities and promotes future developments. å¯é€šè¿‡ `https://github.com/princeton-nlp/HELMET` è®¿é—®ä»£ç å’Œæ•°æ®ã€‚ Code & Data is available at `https://github.com/princeton-nlp/HELMET`.

**Abstract**

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æå¤§åœ°æ‰©å±•äº†å…¶ä¸Šä¸‹æ–‡çª—å£ï¼Œä½†ç°æœ‰è¯„æµ‹ä½“ç³»å­˜åœ¨å±€é™æ€§ã€‚
Large language models (LLMs) have significantly expanded their context window, but existing evaluation methods have limitations.

ç°æœ‰åŸºå‡†æµ‹è¯•æ˜¾ç¤ºå‡ºåç›´è§‰çš„è¶‹åŠ¿ï¼Œä¾‹å¦‚è¾ƒå°çš„æ¨¡å‹ä¼˜äºè¾ƒå¤§çš„æ¨¡å‹ (ä¾‹å¦‚ï¼ŒLlama-3 8B > 70B)ã€‚
Existing benchmarks show counterintuitive trends, such as smaller models outperforming larger ones (e.g., Llama-3 8B > 70B).

ç°æœ‰çš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ (LCLMs) çš„è¯„æµ‹è¿‡åº¦ä¾èµ–äºåˆæˆä»»åŠ¡ï¼Œä¸”æ¨¡å‹å¼€å‘è€…ç»å¸¸ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†è¿›è¡Œè¯„æµ‹ã€‚
Existing evaluations of long-context language models (LCLMs) overly rely on synthetic tasks, and model developers often evaluate on different sets of datasets.

æˆ‘ä»¬æå‡ºäº†HELMET (How to Evaluate Long-Context Models Effectively and Thoroughly)ï¼Œä¸€ä¸ªå…¨é¢çš„LCLMsè¯„æµ‹åŸºå‡†ï¼Œä»¥è§£å†³ç°æœ‰åŸºå‡†çš„å¤šæ ·æ€§ã€å¯æ§æ€§å’Œå¯é æ€§é—®é¢˜ã€‚
We propose HELMET (How to Evaluate Long-Context Models Effectively and Thoroughly), a comprehensive benchmark for evaluating LCLMs that addresses the diversity, controllability, and reliability issues of existing benchmarks.

é€šè¿‡è¯„ä¼°59ä¸ªæœ€æ–°çš„LCLMsï¼Œæˆ‘ä»¬å‘ç°ï¼Œè·¨å¤šæ ·åŒ–åº”ç”¨è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå¹¶ä¸”å‰æ²¿çš„LCLMsåœ¨å¤æ‚ä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚
By evaluating 59 recent LCLMs, we find that it is crucial to evaluate models across diverse applications to understand their capabilities, and frontier LCLMs are still limited on complex tasks.

**Abstract:**

é•¿æ–‡æœ¬è¯­è¨€æ¨¡å‹(LCLMs)çš„å¸¸ç”¨è¯„ä¼°æ–¹æ³•æ˜¯å›°æƒ‘åº¦æˆ–å¦‚å¤§æµ·æé’ˆ(NIAH)ç­‰åˆæˆä»»åŠ¡ã€‚
A common practice for evaluating long-context language models (LCLMs) is to use perplexity or synthetic tasks, such as needle-in-a-haystack (NIAH).

ç„¶è€Œï¼Œè¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå›°æƒ‘åº¦ä¸ä¸‹æ¸¸æ€§èƒ½ç›¸å…³æ€§ä¸ä½³(Fang et al., 2024)ã€‚
However, recent works have shown that perplexity does not correlate well with downstream performance (Fang et al., 2024).

NIAHç­‰ç®€å•åˆæˆä»»åŠ¡ä¸å®é™…ä»»åŠ¡ç›¸å…³æ€§å·®ï¼Œè€Œæ›´å¤æ‚çš„åˆæˆä»»åŠ¡ç›¸å…³æ€§æ›´é«˜ã€‚
Simple synthetic tasks, such as NIAH, do not correlate well with real-world performance, but the more complex synthetic tasks achieve higher correlation with real-world tasks.

ç°æœ‰å®é™…åº”ç”¨åŸºå‡†æµ‹è¯•å¦‚ZeroScrolls, LongBench, å’ŒInfiniteBenchä»å­˜åœ¨ä¸‹æ¸¸ä»»åŠ¡è¦†ç›–ä¸è¶³ã€æµ‹è¯•é•¿åº¦å—é™å’ŒæŒ‡æ ‡ä¸å¯é ç­‰é—®é¢˜ã€‚
Existing benchmarks with realistic applications such as ZeroScrolls, LongBench, and InfiniteBench still face limitations: insufficient task coverage, inadequate length, and unreliable metrics.

å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†HELMETä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œæä¾›å…¨é¢çš„LCLMè¯„ä¼°ã€‚
Thus, we propose HELMET to address these limitations and provide a comprehensive evaluation of LCLMs.

HELMETçš„è®¾è®¡ç›®æ ‡æ˜¯æä¾›å¤šæ ·åŒ–çš„ä¸‹æ¸¸ä»»åŠ¡è¦†ç›–ï¼Œå¯æ§çš„é•¿åº¦å’Œå¤æ‚åº¦ï¼Œä»¥åŠå¯é çš„è¯„ä¼°ï¼Œæ”¯æŒåŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ã€‚
HELMET is designed with diverse downstream task coverage, controllable length/complexity, and reliable evaluation for both base and instruction-tuned models.

æˆ‘ä»¬å®éªŒè¯„ä¼°äº†8Kåˆ°128K tokensçš„è¾“å…¥é•¿åº¦ï¼ŒHELMETå¯æ‰©å±•åˆ°æ›´é•¿æ–‡æœ¬ã€‚
In our experiments, we evaluate on input length from 8K to 128K tokens, and HELMET is easily extended.

## HELMETåŸºå‡†æµ‹è¯•é›†ï¼šå­¦æœ¯æ‘˜è¦

HELMET includes diverse tasks such as retrieval-augmented generation, citation, and summarization.
HELMETåŒ…å«å¤šç§ä»»åŠ¡ï¼Œä¾‹å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆã€å¼•æ–‡ç”Ÿæˆå’Œæ‘˜è¦ç”Ÿæˆã€‚

We select datasets with naturally long contexts for real-world applications.
æˆ‘ä»¬é€‰æ‹©å…·æœ‰è‡ªç„¶é•¿ä¸Šä¸‹æ–‡çš„æ•°æ®é›†ï¼Œä»¥é€‚åº”çœŸå®ä¸–ç•Œçš„åº”ç”¨ã€‚

These datasets are complemented with model-based evaluations and human studies for reliable evaluation.
è¿™äº›æ•°æ®é›†è¾…ä»¥åŸºäºæ¨¡å‹çš„è¯„ä¼°å’Œäººå·¥è¯„ä¼°ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å¯é æ€§ã€‚

Input length is controlled by manipulating retrieved passages (RAG, Cite, Re-rank), demonstrations (ICL), or document length (LongQA, Summ).
è¾“å…¥é•¿åº¦å¯ä»¥é€šè¿‡æ“çºµæ£€ç´¢åˆ°çš„æ®µè½ï¼ˆRAGã€Citeã€Re-rankï¼‰ã€æ¼”ç¤ºï¼ˆICLï¼‰æˆ–æ–‡æ¡£é•¿åº¦ï¼ˆLongQAã€Summï¼‰æ¥æ§åˆ¶ã€‚

LongQA and Summ use datasets with >100K token natural documents.
LongQAå’ŒSummä½¿ç”¨åŒ…å«è¶…è¿‡10ä¸‡tokençš„è‡ªç„¶æ–‡æ¡£çš„æ•°æ®é›†ã€‚

We employ model-based evaluations, showing better distinguishability compared to n-gram metrics (ROUGE). *Figure 3*.
æˆ‘ä»¬é‡‡ç”¨åŸºäºæ¨¡å‹çš„è¯„ä¼°ï¼Œç›¸æ¯”äºåŸºäºn-gramçš„æŒ‡æ ‡ï¼ˆROUGEï¼‰ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„åŒºåˆ†åº¦ã€‚*å›¾3*ã€‚

Human studies show high agreement with our evaluation metrics.
äººå·¥è¯„ä¼°æ˜¾ç¤ºå‡ºä¸æˆ‘ä»¬çš„è¯„ä¼°æŒ‡æ ‡é«˜åº¦ä¸€è‡´ã€‚

We support base models with in-context learning, improving their performance on our tasks.
æˆ‘ä»¬é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ æ”¯æŒåŸºå‡†æ¨¡å‹ï¼Œæé«˜å®ƒä»¬åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**æ‘˜è¦ï¼ˆAbstractï¼‰**

é•¿æ–‡æœ¬è¯­è¨€æ¨¡å‹ï¼ˆLCLMsï¼‰åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­ä»æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚
*LCLMs still have a long way to go on real-world applications.*

æˆ‘ä»¬çš„å®éªŒå’Œåˆ†æåŒ…æ‹¬59ä¸ªLCLMsçš„å…¨é¢é›†åˆã€‚
*Our experiments and analyses include a comprehensive set of 59 LCLMs.*

æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯å¯¹ä¸åŒåº”ç”¨ä¸Šé•¿æ–‡æœ¬æ¨¡å‹æœ€å½»åº•å’Œå—æ§çš„æ¯”è¾ƒã€‚
*To our knowledge, this is the most thorough and controlled comparison of long-context models on diverse applications.*

è¯„ä¼°é•¿æ–‡æœ¬èƒ½åŠ›éœ€è¦å¤šæ ·åŒ–çš„è¯„ä¼°æ–¹å¼ã€‚
*Diverse evaluation is needed for assessing long-context abilities.*

é•¿æ–‡æœ¬åŸºå‡†é€šå¸¸æ˜¯é’ˆå¯¹ç‰¹å®šåº”ç”¨æ„å»ºçš„ï¼Œé™åˆ¶äº†LCLMsåœ¨æ›´å¹¿æ³›ä¸Šä¸‹æ–‡ä¸­çš„ç†è§£ã€‚
*Long-context benchmarks are often constructed with specific applications in mind, which limits the understanding of LCLMs in a broader context.*

æˆ‘ä»¬å‘ç°ä¸åŒç±»åˆ«ä¹‹é—´çš„è¡¨ç°å¹¶ä¸æ€»æ˜¯ç›¸äº’å…³è”ï¼ˆå¦‚å›¾4ï¼‰ã€‚
*We examine model performance over a wide range of real tasks and find that different categories do not always correlate with each other (Figure 4).*

æ¨¡å‹æ€§èƒ½éšç€é•¿åº¦çš„å¢åŠ å’Œä»»åŠ¡å¤æ‚æ€§çš„æé«˜è€Œä¸‹é™ã€‚
*Models degrade with increasing lengths and task complexity.*

æˆ‘ä»¬å±•ç¤ºäº†å‰æ²¿ä¸“æœ‰æ¨¡å‹ä»¥åŠä¸€äº›å¼€æºæ¨¡å‹åœ¨HELMETä¸Šçš„ç»“æœï¼ˆå¦‚å›¾5ï¼‰ã€‚
*We present the results of the frontier proprietary models as well as a few open-source models on HELMET (Figure 5).*

æˆ‘ä»¬è§‚å¯Ÿåˆ°å¼€æºæ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šè½åäºé—­æºæ¨¡å‹ã€‚
*First, we observe that open-source models lag behind closed-source models on complex tasks.*

**Abstract:**

æˆ‘ä»¬ä»‹ç»HELMETï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°é•¿ä¸Šä¸‹æ–‡æ¨¡å‹æ€§èƒ½çš„ç»¼åˆæ¡†æ¶ã€‚
We introduce HELMET, a comprehensive framework for evaluating the performance of long-context models.

HELMETæ­ç¤ºäº†é•¿ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹æ€§èƒ½é€€åŒ–çš„é—®é¢˜ï¼Œä¾‹å¦‚åœ¨é‡æ’åºä»»åŠ¡ä¸­ã€‚
HELMET reveals performance degradation with increasing context lengths, such as in re-ranking tasks.

æ€§èƒ½é€€åŒ–ç¨‹åº¦å–å†³äºä»»åŠ¡ç±»å‹ã€‚
Performance degradation is category-dependent.

å³ä½¿æ˜¯GPT-4oå’ŒGeminiç­‰æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚
Even state-of-the-art models like GPT-4o and Gemini experience a significant performance decrease.

ä¸åŒä»»åŠ¡ç±»åˆ«æ²¡æœ‰æ˜ç¡®çš„â€œæœ€ä½³â€æ¨¡å‹ï¼Œéœ€è¦è·¨ä¸åŒç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚
There is no clear winner across all categories, thereby calling for evaluation across different axes.

HELMETæ˜“äºä½¿ç”¨ï¼Œæ”¯æŒå¤šç§æ¨¡å‹åŠ è½½æ–¹å¼ï¼ŒåŒ…æ‹¬`transformers`ã€TGIã€Inference Endpointsã€`vllm` ä»¥åŠæ¨¡å‹æä¾›å•†çš„APIã€‚
HELMET is easy to use and supports various model loading methods, including `transformers`, TGI, Inference Endpoints, `vllm`, and model provider APIs.

é€šè¿‡ç®€å•çš„å‘½ä»¤è¡Œ `python eval.py --config configs/rag.yaml --model_name_or_path <model_name>` å³å¯è¿è¡Œè¯„ä¼°ã€‚
Evaluations can be run with a simple command: `python eval.py --config configs/rag.yaml --model_name_or_path <model_name>`.

**Abstract:**

æœ¬æ–‡æ¡£æ¦‚è¿°äº†ä½¿ç”¨å¤šç§æ¨ç†æœåŠ¡è¿›è¡Œè¯„ä¼°çš„æ–¹æ³•ã€‚
This document outlines methods for evaluation using various inference services.

åŒ…æ‹¬é€šè¿‡ Text Generation Inference (TGI) (éœ€è¦ `"tgi:"` å‰ç¼€ and `use_tgi_serving: true`)ã€HuggingFace Inference Endpointsã€VLLM (`use_vllm_serving: true`)ä»¥åŠ OpenAIã€Anthropicã€Google å’Œ TogetherAI ç­‰æ¨¡å‹æä¾›å•†çš„ API è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚
Model evaluation is possible via Text Generation Inference (TGI) (requiring `"tgi:"` prefix and `use_tgi_serving: true`), HuggingFace Inference Endpoints, VLLM (`use_vllm_serving: true`), and Model Provider APIs (OpenAI, Anthropic, Google, TogetherAI).

é€šè¿‡è®¾ç½® `LLM_ENPOINT` å’Œ API å¯†é’¥ï¼Œå¹¶è¿è¡Œ `python eval.py --config configs/config.yaml --endpoint_url $LLM_ENDPOINT [--api_key $API_KEY]` æŒ‡ä»¤æ¥å¯åŠ¨è¯„æµ‹ã€‚
Evaluation is initiated by setting `LLM_ENPOINT` and API keys, then running `python eval.py --config configs/config.yaml --endpoint_url $LLM_ENDPOINT [--api_key $API_KEY]`.

æ¨èä½¿ç”¨ Recall å’Œ RAG ä»»åŠ¡åŠ é€Ÿæ¨¡å‹å¼€å‘ (`python eval.py --config configs/rag.yaml --model_name_or_path <model_name>`).
Recall and RAG tasks are recommended for faster model development (`python eval.py --config configs/rag.yaml --model_name_or_path <model_name>`).

**Abstract**

é•¿æ–‡æœ¬è¯­è¨€æ¨¡å‹ï¼ˆLCLMsï¼‰çš„è¯„ä¼°ï¼Œå°¤å…¶æ˜¯åœ¨é•¿ä¸Šä¸‹æ–‡æƒ…å†µä¸‹ï¼Œé‰´äºå…¶è®¡ç®—å’Œå†…å­˜æˆæœ¬ï¼Œæå…·æŒ‘æˆ˜æ€§ã€‚
Evaluating Long-Context Language Models (LCLMs), especially at long contexts, is challenging given their computational and memory costs.

ä¾‹å¦‚ï¼Œåœ¨70Bæ¨¡å‹ä¸Šè¿è¡Œæ‰€æœ‰é•¿åº¦çš„HELMETéœ€è¦ä¸€ä¸ªå…·æœ‰8 * 80GB GPUçš„èŠ‚ç‚¹ï¼Œè€—è´¹æ•°ç™¾GPUå°æ—¶ï¼Œæˆæœ¬é«˜æ˜‚ã€‚
For example, running HELMET at all lengths on a 70B model requires a node with 8 * 80GB GPUs for hundreds of GPU hours, which can be costly.

é€šè¿‡åœ¨HELMETä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶äººå‘˜åªéœ€å‚è€ƒæˆ‘ä»¬çš„ç»“æœï¼Œå³å¯ç›´æ¥å°†ä»–ä»¬çš„æ¨¡å‹ä¸ç°æœ‰çš„59ä¸ªä¸åŒå¤§å°å’Œæ¶æ„çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
By evaluating on HELMET, researchers can directly compare their models to existing ones simply by referencing our results, which cover 59 models of different sizes and architectures.

æˆ‘ä»¬æœ€è¿‘å‘å¸ƒäº†LongProcï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°LCLMsåœ¨é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆlong-form generationï¼‰å’Œéµå¾ªç¨‹åºï¼ˆfollowing proceduresï¼‰æ–¹é¢çš„åŸºå‡†ã€‚
We recently released LongProc, a benchmark for evaluating LCLMs on long-form generation and following procedures.

LongProcä¾§é‡äºæ›´é•¿çš„è¾“å‡ºï¼Œé«˜è¾¾8K tokensï¼Œè€Œä¸ä»…ä»…æ˜¯æ‘˜è¦ä»»åŠ¡çš„1K tokensè¾“å‡ºã€‚
LongProc focuses on even longer outputs, up to 8K tokens, unlike summarization tasks with up to 1K tokens outputs.

æˆ‘ä»¬æ­£åœ¨åŠªåŠ›å°†LongProcæ•´åˆåˆ°HELMETçš„è¯„ä¼°å¥—ä»¶ä¸­ï¼Œå¸Œæœ›è¿™å°†ä¸ºLCLMsåœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šæä¾›æ›´å…¨é¢çš„è¯„ä¼°ã€‚
We are working on integrating LongProc into HELMET's evaluation suite, and we hope that this will provide a more comprehensive evaluation of LCLMs on long-form tasks.

```
**ä¸­è‹±å¯¹ç…§æ‘˜è¦ï¼ˆAbstract - Bilingual Abstractï¼‰**

æœ¬æ–‡æ¦‚è¿°äº†è¿‘æœŸå…³äºå¤§å‹è¯­è¨€æ¨¡å‹(LLM) çš„è®¨è®ºï¼Œé‡ç‚¹å…³æ³¨Google Gemma 3å’ŒModernBERTã€‚
This abstract outlines recent discussions on Large Language Models (LLMs), highlighting Google's Gemma 3 and ModernBERT.

Gemma 3è¢«æè¿°ä¸ºå¤šæ¨¡æ€ã€å¤šè¯­è¨€ã€é•¿ä¸Šä¸‹æ–‡çš„å¼€æºLLMã€‚
Gemma 3 is described as a multimodal, multilingual, long-context open-source LLM.

ModernBERT åˆ™è¢«è§†ä¸ºBERTçš„æ›¿ä»£æ–¹æ¡ˆã€‚
ModernBERT is presented as a replacement for BERT.

è¯„è®ºåŒºæåˆ°äº†ä¸€ä¸ªè¯„ä¼°é•¿ä¸Šä¸‹æ–‡çš„æ–°åŸºå‡†ï¼šNoLiMaï¼Œç›¸å…³è®ºæ–‡é“¾æ¥ä¸ºhttps://github.com/adobe-research/NoLiMaã€‚
A comment mentions a new benchmark for evaluating long contexts: NoLiMa, with the related paper available at https://github.com/adobe-research/NoLiMa.

è¯¥åŸºå‡†è¶…è¶Šäº†å­—é¢åŒ¹é…è¿›è¡Œé•¿ä¸Šä¸‹æ–‡è¯„ä¼°ã€‚
This benchmark goes beyond literal matching for long-context evaluation.

æ€»ä½“è€Œè¨€ï¼Œè®¨è®ºå›´ç»•ç€LLMçš„æœ€æ–°è¿›å±•å’Œè¯„ä¼°æ–¹æ³•å±•å¼€ã€‚
Overall, the discussion revolves around the latest advancements and evaluation methods in LLMs.
```

---

